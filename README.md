# LLM-Architecture-Evaluation
This repository contains code and analysis for the Build LLM: Research Based Assessment assignment, conducted under the Program Elective course Build GPT from Scratch offered by Vizuara.
Overview

The objective of this project is to investigate the effects of various architectural components and hyperparameters on the training dynamics and output quality of Large Language Models (LLMs) trained from scratch. The work includes both baseline implementation and ablation studies, providing empirical insight into transformer model design choices.

Team Members
Yash Kambli â€“ yash.kambli22@spit.ac.in
Ayush Nemade - ayush.nemade22@spit.ac.in

## Description

This project investigates the design and training dynamics of transformer-based Large Language Models (LLMs) from scratch. By systematically modifying key architectural components and hyperparameters, the study aims to understand their impact on model performance, stability, and generalization.

All experiments were conducted on a dataset composed of classic English literature sourced from Project Gutenberg. The project includes both baseline training and ablation studies to evaluate the role of normalization layers, residual connections, and feedforward networks.

## License

This repository is intended for academic purposes only as part of coursework under the **Build LLM from Scratch** program elective offered by **Vizuara** at Sardar Patel Institute of Technology, Mumbai. The dataset used consists of texts in the public domain.

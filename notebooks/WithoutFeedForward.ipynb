{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ6a7hFfdtBC",
        "outputId": "a4054199-0da6-4c3a-de56-4bc07bfc479b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'data.txt' loaded with 1935114 characters.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "file_path = \"data.txt\"\n",
        "\n",
        "# Check if the file exists in the current directory (i.e., Colab session)\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()\n",
        "    print(f\"File '{file_path}' loaded with {len(text_data)} characters.\")\n",
        "else:\n",
        "    print(f\"File '{file_path}' not found. Please upload it to the Colab environment first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tiktoken > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "JvoZA0hmd9Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjQRM_byd-yO",
        "outputId": "b15d4ffd-66ca-4dfa-9b49-efd147036af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "zTZQhETUeB5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_ssuT0-eErm",
        "outputId": "fe3953b8-6ef5-4a31-b65a-6eb93b47acb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "integers = tokenizer.encode(\"Akwirw ier\")\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV-tQkameFLH",
        "outputId": "6de7bed7-3c89-4a06-989a-6a6816c77507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n",
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"Characters:\", total_characters)\n",
        "print(\"Tokens:\", total_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaoJJ1EkeG3W",
        "outputId": "a4cbdd11-b1c0-4939-9801-bca6fa0c8184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters: 1935114\n",
            "Tokens: 522304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlaWZ3SJeHwd",
        "outputId": "976e3a9a-d8a8-41ca-8a85-79e8e455babf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "522304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "L6OjyL07eIgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "_1PvIyYSeJgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u63WM6e7eKac",
        "outputId": "1f2aa1b1-6b8b-4a9b-f5a3-7d4a06d5c609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "[tensor([[   62,    54,  2501, 47958]]), tensor([[   54,  2501, 47958,   468]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n"
      ],
      "metadata": {
        "id": "2JzV78qQeLS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ],
      "metadata": {
        "id": "IF9OhuIveMGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngw1-FHReM0H",
        "outputId": "69d28cd1-d9a3-418d-cd6c-2aecae2547e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   62,    54,  2501, 47958],\n",
            "        [  468,  7382,   257,  3734],\n",
            "        [  290,   655, 12941,  1022],\n",
            "        [  564,   250, 33983,   198],\n",
            "        [ 1525, 24930,   447,   251],\n",
            "        [  290,   564,   250, 33983],\n",
            "        [  351,  2614,  1842,    13],\n",
            "        [  447,   251,   770, 12941]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QTvNMtneNjw",
        "outputId": "929b5a30-1ec3-4dfa-b75e-6fc16626bc2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ],
      "metadata": {
        "id": "GgH8dHb4eOnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBx3nsshePaW",
        "outputId": "26111ae1-ed6c-482f-918b-075c84c02fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsdrxJoseQc8",
        "outputId": "3acf8a1b-af01-4ee0-fba3-3fe2fbe44a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "SWHjZtAUeRUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# Define the tensor with 3 rows and 6 columns\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
        "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
        "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 6\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjDNV2ngeSO5",
        "outputId": "c4e57ce4-4d84-4da2-c09e-a39143bbf9fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
            "\n",
            "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 3, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 256, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "Y8v9rmgueTXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
        "            GELU(), ## Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "QHem_9yweUJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"]\n",
        "        )\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block only\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Residual connection\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "mnphNxoheVCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "4I_SVXkAeWA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSZOVIGmeXCP",
        "outputId": "f7fa1503-03e2-4224-9135-9d7ae441c067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[ 0.1028, -0.0475,  0.0510,  ..., -1.6750,  1.4850, -0.1469],\n",
            "         [ 0.1619, -0.7593,  0.3201,  ..., -0.1294,  0.2091, -0.2601],\n",
            "         [-0.4534, -0.4157, -0.3908,  ..., -0.4465,  0.8000, -0.2343],\n",
            "         [-0.1794,  0.5242, -0.6918,  ...,  1.1994,  0.1436, -0.6666]],\n",
            "\n",
            "        [[ 0.0243,  0.0092,  0.3374,  ..., -1.7356,  1.2881, -0.0789],\n",
            "         [ 0.2340, -1.0713,  0.0424,  ..., -0.5413,  0.0162,  0.2881],\n",
            "         [ 0.3359, -0.2239,  0.4708,  ...,  0.2192, -0.2852, -0.0442],\n",
            "         [ 0.1269, -0.1410, -0.0713,  ...,  1.8261,  0.6507, -0.2269]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxpfC6NfeYIz",
        "outputId": "8d4737d2-b3b0-43cd-ed4f-32e07c12b740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 105,732,096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "print(\"Output layer shape:\", model.out_head.weight.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmuEJr2CeY4v",
        "outputId": "35dced42-40cc-4c32-f3f0-62a8f2edb801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embedding layer shape: torch.Size([50257, 768])\n",
            "Output layer shape: torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_size_bytes = total_params * 4 #A\n",
        "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
        "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP-gCGG3eZuH",
        "outputId": "f383f671-c876-4f6d-f096-31814d70f2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model: 403.34 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "\n",
        "    ###Input batch:\n",
        " ###tensor([[6109, 3626, 6100,  345],\n",
        "        ##[6109, 1110, 6622,  257]])\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "dpfRj0_geakt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"He said we came here\"\n",
        "\n",
        "\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oBhY9blebh1",
        "outputId": "3a844da3-53e7-4245-8f5e-5120d427aee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " He said we came hereremoteeksshences)/ trapped562 ranged congestProxy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "KiQ0V_j7ecnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "s-2R-xMPedwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Note:\n",
        "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
        "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
        "# However, the resulting loss values may be slightly different.\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = torch.device(\"cuda\")\n",
        "#elif torch.backends.mps.is_available():\n",
        "#    device = torch.device(\"mps\")\n",
        "#else:\n",
        "#    device = torch.device(\"cpu\")\n",
        "#\n",
        "# print(f\"Using {device} device.\")\n",
        "\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqEYZ5P0eek3",
        "outputId": "8142c960-b0e5-473d-bcd0-8b9917472a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.998184091719237\n",
            "Validation loss: 11.005849193882298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gqPsx5Oefk7",
        "outputId": "ac349a0a-daaf-4800-b271-0fbd7ba2907d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer,\n",
        "                       store_epochs={5, 10, 20, 50, 100, 200, 500},\n",
        "                       save_path=\"intermediate_results.json\"):\n",
        "\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    intermediate_results = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"tokens_seen\": []}\n",
        "\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    def save_results_to_disk(results, path):\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(1, num_epochs + 1):  # Epoch starts at 1\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward()  # Calculate loss gradients\n",
        "            optimizer.step()  # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Store intermediate results if this epoch is in store_epochs\n",
        "        if epoch in store_epochs:\n",
        "            # Perform evaluation if not already done in last step of epoch\n",
        "            if not (global_step % eval_freq == 0):\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "\n",
        "            intermediate_results[\"epoch\"].append(epoch)\n",
        "            intermediate_results[\"train_loss\"].append(train_loss)\n",
        "            intermediate_results[\"val_loss\"].append(val_loss)\n",
        "            intermediate_results[\"tokens_seen\"].append(tokens_seen)\n",
        "            save_results_to_disk(intermediate_results, save_path)\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n"
      ],
      "metadata": {
        "id": "wz7MJDyRegaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "Uw7TdCxIehUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "JOSPqvH2eiCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "RESULT_DIR = \"/content/drive/MyDrive/grid_search_results\"\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-spvnylercU",
        "outputId": "7f68a5c9-d0e9-40f5-c8c3-f815c8c5c1d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import torch\n",
        "import time\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Dark background settings\n",
        "plt.style.use('dark_background')\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "# ---- Smoothing Function ----\n",
        "def smooth(values, window_size=20):\n",
        "    if len(values) < window_size:\n",
        "        return values\n",
        "    return np.convolve(values, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "# ---- Dark Themed Plotting Function ----\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses, lr_label, epoch_count, smooth_window=20):\n",
        "    smoothed_train = smooth(train_losses, smooth_window)\n",
        "    smoothed_val = smooth(val_losses, smooth_window)\n",
        "\n",
        "    offset = (len(train_losses) - len(smoothed_train)) // 2\n",
        "    epochs_smoothed = epochs_seen[offset : offset + len(smoothed_train)]\n",
        "    tokens_smoothed = tokens_seen[offset : offset + len(smoothed_train)]\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(7, 4))\n",
        "    fig.patch.set_facecolor('black')\n",
        "    ax1.set_facecolor('black')\n",
        "\n",
        "    ax1.plot(epochs_smoothed, smoothed_train, label=\"Training Loss\", color=\"cyan\", linewidth=2)\n",
        "    ax1.plot(epochs_smoothed, smoothed_val, label=\"Validation Loss\", color=\"yellow\", linewidth=2, linestyle=\"--\")\n",
        "\n",
        "    ax1.set_xlabel(\"Epochs\", fontsize=12, color='white')\n",
        "    ax1.set_ylabel(\"Loss\", fontsize=12, color='white')\n",
        "    ax1.tick_params(colors='white')\n",
        "    ax1.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "\n",
        "    ax2 = ax1.twiny()\n",
        "    ax2.plot(tokens_smoothed, smoothed_train, alpha=0)\n",
        "    ax2.set_xlabel(\"Tokens Seen\", fontsize=12, color='white')\n",
        "    ax2.tick_params(colors='white')\n",
        "\n",
        "    fig.suptitle(f\"Loss Curve (LR={lr_label}, Epochs={epoch_count})\", fontsize=14, color='white', weight='bold')\n",
        "    legend = ax1.legend(loc=\"upper right\", fontsize=10)\n",
        "    for text in legend.get_texts():\n",
        "        text.set_color(\"white\")\n",
        "\n",
        "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plot_filename = os.path.join(RESULT_DIR, f\"no_feedforward.pdf\")\n",
        "    plt.savefig(plot_filename, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "# ---- Training Wrapper Function ----\n",
        "def run_training_for_epochs(lr, epoch_count):\n",
        "    print(f\"\\nTraining with learning rate: {lr} for {epoch_count} epochs\")\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    model = GPTModel(GPT_CONFIG_124M)\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.1)\n",
        "\n",
        "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "        model, train_loader, val_loader, optimizer, device,\n",
        "        num_epochs=epoch_count, eval_freq=5, eval_iter=1,\n",
        "        start_context=\"He said we came here\", tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    epochs_tensor = torch.linspace(0, epoch_count, len(train_losses))\n",
        "    lr_label = str(lr).replace(\".\", \"_\")\n",
        "    plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses, lr_label, epoch_count)\n",
        "\n",
        "    token_ids = text_to_token_ids(\"The tree was\", tokenizer).to(device)\n",
        "    token_ids = generate_text_simple(\n",
        "        model=model,\n",
        "        idx=token_ids,\n",
        "        max_new_tokens=25,\n",
        "        context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        "    )\n",
        "    output_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "    output_file = os.path.join(RESULT_DIR, f\"output_text_no_feedforward.txt\")\n",
        "    with open(output_file, \"w\") as f:\n",
        "        f.write(output_text)\n",
        "\n",
        "    print(\"Sample output:\\n\", output_text)\n",
        "\n",
        "    loss_file = os.path.join(RESULT_DIR, f\"final_losses_no_feedforward.txt\")\n",
        "    with open(loss_file, \"w\") as f:\n",
        "        f.write(f\"Final Training Loss: {train_losses[-1]:.4f}\\n\")\n",
        "        f.write(f\"Final Validation Loss: {val_losses[-1]:.4f}\\n\")\n",
        "\n",
        "# ---- Run Training ----\n",
        "learning_rate = 0.0001\n",
        "epoch_variations = [10]\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch_count in epoch_variations:\n",
        "    run_training_for_epochs(learning_rate, epoch_count)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nAll training runs completed in {(end_time - start_time)/60:.2f} minutes.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obttIfzNeseA",
        "outputId": "73cd2c63-bfb1-45fa-fda6-de138261cc25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with learning rate: 0.0001 for 10 epochs\n",
            "Ep 1 (Step 000000): Train loss 10.937, Val loss 10.943\n",
            "Ep 1 (Step 000005): Train loss 9.938, Val loss 10.014\n",
            "Ep 1 (Step 000010): Train loss 9.406, Val loss 9.086\n",
            "Ep 1 (Step 000015): Train loss 8.925, Val loss 8.619\n",
            "Ep 1 (Step 000020): Train loss 8.420, Val loss 8.214\n",
            "Ep 1 (Step 000025): Train loss 8.015, Val loss 7.845\n",
            "Ep 1 (Step 000030): Train loss 7.742, Val loss 7.558\n",
            "Ep 1 (Step 000035): Train loss 7.194, Val loss 7.340\n",
            "Ep 1 (Step 000040): Train loss 7.521, Val loss 7.166\n",
            "Ep 1 (Step 000045): Train loss 6.693, Val loss 7.061\n",
            "Ep 1 (Step 000050): Train loss 7.118, Val loss 6.991\n",
            "Ep 1 (Step 000055): Train loss 7.126, Val loss 6.979\n",
            "Ep 1 (Step 000060): Train loss 6.846, Val loss 6.947\n",
            "Ep 1 (Step 000065): Train loss 6.998, Val loss 6.925\n",
            "Ep 1 (Step 000070): Train loss 6.880, Val loss 6.956\n",
            "Ep 1 (Step 000075): Train loss 6.449, Val loss 6.994\n",
            "Ep 1 (Step 000080): Train loss 6.724, Val loss 6.937\n",
            "Ep 1 (Step 000085): Train loss 6.606, Val loss 6.900\n",
            "Ep 1 (Step 000090): Train loss 6.778, Val loss 6.900\n",
            "Ep 1 (Step 000095): Train loss 6.621, Val loss 6.912\n",
            "Ep 1 (Step 000100): Train loss 7.159, Val loss 6.918\n",
            "Ep 1 (Step 000105): Train loss 6.918, Val loss 6.905\n",
            "Ep 1 (Step 000110): Train loss 6.586, Val loss 6.890\n",
            "Ep 1 (Step 000115): Train loss 6.304, Val loss 6.845\n",
            "Ep 1 (Step 000120): Train loss 6.768, Val loss 6.841\n",
            "Ep 1 (Step 000125): Train loss 6.892, Val loss 6.829\n",
            "Ep 1 (Step 000130): Train loss 6.402, Val loss 6.814\n",
            "Ep 1 (Step 000135): Train loss 6.655, Val loss 6.793\n",
            "Ep 1 (Step 000140): Train loss 6.594, Val loss 6.747\n",
            "Ep 1 (Step 000145): Train loss 6.080, Val loss 6.739\n",
            "Ep 1 (Step 000150): Train loss 6.546, Val loss 6.757\n",
            "Ep 1 (Step 000155): Train loss 6.639, Val loss 6.772\n",
            "Ep 1 (Step 000160): Train loss 6.749, Val loss 6.787\n",
            "Ep 1 (Step 000165): Train loss 6.340, Val loss 6.737\n",
            "Ep 1 (Step 000170): Train loss 6.320, Val loss 6.649\n",
            "Ep 1 (Step 000175): Train loss 6.325, Val loss 6.610\n",
            "Ep 1 (Step 000180): Train loss 6.218, Val loss 6.561\n",
            "Ep 1 (Step 000185): Train loss 6.678, Val loss 6.448\n",
            "Ep 1 (Step 000190): Train loss 6.384, Val loss 6.465\n",
            "Ep 1 (Step 000195): Train loss 6.120, Val loss 6.448\n",
            "Ep 1 (Step 000200): Train loss 6.237, Val loss 6.409\n",
            "Ep 1 (Step 000205): Train loss 6.087, Val loss 6.365\n",
            "Ep 1 (Step 000210): Train loss 6.381, Val loss 6.391\n",
            "Ep 1 (Step 000215): Train loss 5.639, Val loss 6.372\n",
            "Ep 1 (Step 000220): Train loss 6.583, Val loss 6.345\n",
            "Ep 1 (Step 000225): Train loss 6.145, Val loss 6.300\n",
            "Ep 1 (Step 000230): Train loss 5.991, Val loss 6.306\n",
            "Ep 1 (Step 000235): Train loss 6.028, Val loss 6.294\n",
            "Ep 1 (Step 000240): Train loss 6.438, Val loss 6.288\n",
            "Ep 1 (Step 000245): Train loss 6.268, Val loss 6.300\n",
            "Ep 1 (Step 000250): Train loss 6.032, Val loss 6.281\n",
            "Ep 1 (Step 000255): Train loss 5.592, Val loss 6.216\n",
            "Ep 1 (Step 000260): Train loss 5.957, Val loss 6.167\n",
            "Ep 1 (Step 000265): Train loss 5.756, Val loss 6.153\n",
            "Ep 1 (Step 000270): Train loss 5.836, Val loss 6.171\n",
            "Ep 1 (Step 000275): Train loss 6.160, Val loss 6.149\n",
            "Ep 1 (Step 000280): Train loss 6.518, Val loss 6.116\n",
            "Ep 1 (Step 000285): Train loss 6.174, Val loss 6.083\n",
            "Ep 1 (Step 000290): Train loss 6.041, Val loss 6.090\n",
            "Ep 1 (Step 000295): Train loss 6.257, Val loss 6.079\n",
            "Ep 1 (Step 000300): Train loss 6.583, Val loss 6.060\n",
            "Ep 1 (Step 000305): Train loss 5.950, Val loss 6.089\n",
            "Ep 1 (Step 000310): Train loss 5.996, Val loss 6.055\n",
            "Ep 1 (Step 000315): Train loss 5.846, Val loss 6.063\n",
            "Ep 1 (Step 000320): Train loss 6.159, Val loss 6.073\n",
            "Ep 1 (Step 000325): Train loss 6.181, Val loss 6.063\n",
            "Ep 1 (Step 000330): Train loss 5.682, Val loss 6.020\n",
            "Ep 1 (Step 000335): Train loss 5.822, Val loss 5.995\n",
            "Ep 1 (Step 000340): Train loss 5.399, Val loss 5.999\n",
            "Ep 1 (Step 000345): Train loss 6.141, Val loss 5.996\n",
            "Ep 1 (Step 000350): Train loss 5.355, Val loss 5.945\n",
            "Ep 1 (Step 000355): Train loss 6.174, Val loss 5.896\n",
            "Ep 1 (Step 000360): Train loss 6.360, Val loss 5.908\n",
            "Ep 1 (Step 000365): Train loss 5.348, Val loss 5.943\n",
            "Ep 1 (Step 000370): Train loss 5.489, Val loss 5.922\n",
            "Ep 1 (Step 000375): Train loss 6.115, Val loss 5.893\n",
            "Ep 1 (Step 000380): Train loss 5.811, Val loss 5.895\n",
            "Ep 1 (Step 000385): Train loss 5.648, Val loss 5.898\n",
            "Ep 1 (Step 000390): Train loss 5.555, Val loss 5.873\n",
            "Ep 1 (Step 000395): Train loss 5.804, Val loss 5.864\n",
            "Ep 1 (Step 000400): Train loss 6.341, Val loss 5.899\n",
            "Ep 1 (Step 000405): Train loss 6.299, Val loss 5.895\n",
            "Ep 1 (Step 000410): Train loss 5.329, Val loss 5.853\n",
            "Ep 1 (Step 000415): Train loss 5.898, Val loss 5.839\n",
            "Ep 1 (Step 000420): Train loss 5.190, Val loss 5.834\n",
            "Ep 1 (Step 000425): Train loss 5.337, Val loss 5.839\n",
            "Ep 1 (Step 000430): Train loss 5.878, Val loss 5.815\n",
            "Ep 1 (Step 000435): Train loss 5.477, Val loss 5.832\n",
            "Ep 1 (Step 000440): Train loss 5.712, Val loss 5.812\n",
            "Ep 1 (Step 000445): Train loss 5.803, Val loss 5.802\n",
            "Ep 1 (Step 000450): Train loss 5.516, Val loss 5.805\n",
            "Ep 1 (Step 000455): Train loss 6.039, Val loss 5.822\n",
            "Ep 1 (Step 000460): Train loss 6.594, Val loss 5.838\n",
            "Ep 1 (Step 000465): Train loss 5.792, Val loss 5.798\n",
            "Ep 1 (Step 000470): Train loss 5.402, Val loss 5.770\n",
            "Ep 1 (Step 000475): Train loss 5.674, Val loss 5.771\n",
            "Ep 1 (Step 000480): Train loss 5.479, Val loss 5.766\n",
            "Ep 1 (Step 000485): Train loss 6.076, Val loss 5.767\n",
            "Ep 1 (Step 000490): Train loss 5.223, Val loss 5.797\n",
            "Ep 1 (Step 000495): Train loss 6.245, Val loss 5.762\n",
            "Ep 1 (Step 000500): Train loss 5.183, Val loss 5.738\n",
            "Ep 1 (Step 000505): Train loss 5.487, Val loss 5.737\n",
            "Ep 1 (Step 000510): Train loss 5.422, Val loss 5.696\n",
            "Ep 1 (Step 000515): Train loss 5.300, Val loss 5.727\n",
            "Ep 1 (Step 000520): Train loss 5.596, Val loss 5.738\n",
            "Ep 1 (Step 000525): Train loss 4.898, Val loss 5.781\n",
            "Ep 1 (Step 000530): Train loss 5.962, Val loss 5.736\n",
            "Ep 1 (Step 000535): Train loss 5.557, Val loss 5.694\n",
            "Ep 1 (Step 000540): Train loss 5.544, Val loss 5.673\n",
            "Ep 1 (Step 000545): Train loss 5.278, Val loss 5.666\n",
            "Ep 1 (Step 000550): Train loss 5.713, Val loss 5.667\n",
            "Ep 1 (Step 000555): Train loss 5.402, Val loss 5.678\n",
            "Ep 1 (Step 000560): Train loss 5.408, Val loss 5.675\n",
            "Ep 1 (Step 000565): Train loss 5.529, Val loss 5.681\n",
            "Ep 1 (Step 000570): Train loss 5.862, Val loss 5.702\n",
            "Ep 1 (Step 000575): Train loss 5.719, Val loss 5.686\n",
            "Ep 1 (Step 000580): Train loss 5.441, Val loss 5.683\n",
            "Ep 1 (Step 000585): Train loss 5.725, Val loss 5.680\n",
            "Ep 1 (Step 000590): Train loss 5.136, Val loss 5.672\n",
            "Ep 1 (Step 000595): Train loss 5.351, Val loss 5.665\n",
            "Ep 1 (Step 000600): Train loss 5.372, Val loss 5.658\n",
            "Ep 1 (Step 000605): Train loss 5.667, Val loss 5.654\n",
            "Ep 1 (Step 000610): Train loss 5.419, Val loss 5.642\n",
            "Ep 1 (Step 000615): Train loss 5.153, Val loss 5.660\n",
            "Ep 1 (Step 000620): Train loss 5.289, Val loss 5.673\n",
            "Ep 1 (Step 000625): Train loss 5.715, Val loss 5.688\n",
            "Ep 1 (Step 000630): Train loss 5.179, Val loss 5.615\n",
            "Ep 1 (Step 000635): Train loss 5.365, Val loss 5.583\n",
            "Ep 1 (Step 000640): Train loss 6.153, Val loss 5.610\n",
            "Ep 1 (Step 000645): Train loss 5.549, Val loss 5.586\n",
            "Ep 1 (Step 000650): Train loss 5.746, Val loss 5.598\n",
            "Ep 1 (Step 000655): Train loss 5.132, Val loss 5.575\n",
            "Ep 1 (Step 000660): Train loss 5.495, Val loss 5.560\n",
            "Ep 1 (Step 000665): Train loss 4.960, Val loss 5.528\n",
            "Ep 1 (Step 000670): Train loss 5.724, Val loss 5.543\n",
            "Ep 1 (Step 000675): Train loss 5.506, Val loss 5.559\n",
            "Ep 1 (Step 000680): Train loss 4.481, Val loss 5.553\n",
            "Ep 1 (Step 000685): Train loss 5.429, Val loss 5.542\n",
            "Ep 1 (Step 000690): Train loss 5.297, Val loss 5.546\n",
            "Ep 1 (Step 000695): Train loss 4.809, Val loss 5.503\n",
            "Ep 1 (Step 000700): Train loss 5.285, Val loss 5.518\n",
            "Ep 1 (Step 000705): Train loss 5.216, Val loss 5.484\n",
            "Ep 1 (Step 000710): Train loss 5.233, Val loss 5.512\n",
            "Ep 1 (Step 000715): Train loss 4.927, Val loss 5.525\n",
            "Ep 1 (Step 000720): Train loss 5.337, Val loss 5.544\n",
            "Ep 1 (Step 000725): Train loss 5.466, Val loss 5.569\n",
            "Ep 1 (Step 000730): Train loss 5.158, Val loss 5.516\n",
            "Ep 1 (Step 000735): Train loss 5.240, Val loss 5.560\n",
            "Ep 1 (Step 000740): Train loss 5.089, Val loss 5.536\n",
            "Ep 1 (Step 000745): Train loss 5.045, Val loss 5.552\n",
            "Ep 1 (Step 000750): Train loss 4.940, Val loss 5.538\n",
            "Ep 1 (Step 000755): Train loss 5.369, Val loss 5.522\n",
            "Ep 1 (Step 000760): Train loss 4.550, Val loss 5.493\n",
            "Ep 1 (Step 000765): Train loss 5.299, Val loss 5.493\n",
            "Ep 1 (Step 000770): Train loss 5.452, Val loss 5.510\n",
            "Ep 1 (Step 000775): Train loss 5.682, Val loss 5.487\n",
            "Ep 1 (Step 000780): Train loss 6.276, Val loss 5.542\n",
            "Ep 1 (Step 000785): Train loss 5.257, Val loss 5.493\n",
            "Ep 1 (Step 000790): Train loss 5.339, Val loss 5.490\n",
            "Ep 1 (Step 000795): Train loss 5.261, Val loss 5.497\n",
            "Ep 1 (Step 000800): Train loss 5.351, Val loss 5.468\n",
            "Ep 1 (Step 000805): Train loss 5.419, Val loss 5.475\n",
            "Ep 1 (Step 000810): Train loss 5.305, Val loss 5.466\n",
            "Ep 1 (Step 000815): Train loss 5.846, Val loss 5.476\n",
            "Ep 1 (Step 000820): Train loss 5.236, Val loss 5.499\n",
            "Ep 1 (Step 000825): Train loss 4.934, Val loss 5.503\n",
            "Ep 1 (Step 000830): Train loss 5.259, Val loss 5.449\n",
            "Ep 1 (Step 000835): Train loss 5.037, Val loss 5.455\n",
            "Ep 1 (Step 000840): Train loss 4.802, Val loss 5.454\n",
            "Ep 1 (Step 000845): Train loss 5.126, Val loss 5.438\n",
            "Ep 1 (Step 000850): Train loss 5.213, Val loss 5.448\n",
            "Ep 1 (Step 000855): Train loss 5.308, Val loss 5.423\n",
            "Ep 1 (Step 000860): Train loss 5.457, Val loss 5.460\n",
            "Ep 1 (Step 000865): Train loss 5.221, Val loss 5.437\n",
            "Ep 1 (Step 000870): Train loss 5.353, Val loss 5.476\n",
            "Ep 1 (Step 000875): Train loss 5.035, Val loss 5.436\n",
            "Ep 1 (Step 000880): Train loss 5.725, Val loss 5.461\n",
            "Ep 1 (Step 000885): Train loss 5.191, Val loss 5.465\n",
            "Ep 1 (Step 000890): Train loss 4.621, Val loss 5.459\n",
            "Ep 1 (Step 000895): Train loss 5.044, Val loss 5.447\n",
            "Ep 1 (Step 000900): Train loss 5.237, Val loss 5.433\n",
            "Ep 1 (Step 000905): Train loss 4.844, Val loss 5.446\n",
            "He said we came here. I do not be a I am not know you are not be a  I am not know you. I do not know you are not know,\n",
            "Ep 2 (Step 000910): Train loss 4.783, Val loss 5.407\n",
            "Ep 2 (Step 000915): Train loss 5.085, Val loss 5.447\n",
            "Ep 2 (Step 000920): Train loss 5.544, Val loss 5.438\n",
            "Ep 2 (Step 000925): Train loss 5.225, Val loss 5.487\n",
            "Ep 2 (Step 000930): Train loss 5.072, Val loss 5.468\n",
            "Ep 2 (Step 000935): Train loss 5.107, Val loss 5.443\n",
            "Ep 2 (Step 000940): Train loss 5.374, Val loss 5.440\n",
            "Ep 2 (Step 000945): Train loss 5.067, Val loss 5.436\n",
            "Ep 2 (Step 000950): Train loss 4.726, Val loss 5.462\n",
            "Ep 2 (Step 000955): Train loss 4.900, Val loss 5.432\n",
            "Ep 2 (Step 000960): Train loss 5.078, Val loss 5.494\n",
            "Ep 2 (Step 000965): Train loss 5.256, Val loss 5.450\n",
            "Ep 2 (Step 000970): Train loss 5.691, Val loss 5.419\n",
            "Ep 2 (Step 000975): Train loss 5.421, Val loss 5.410\n",
            "Ep 2 (Step 000980): Train loss 4.954, Val loss 5.422\n",
            "Ep 2 (Step 000985): Train loss 5.201, Val loss 5.413\n",
            "Ep 2 (Step 000990): Train loss 5.341, Val loss 5.422\n",
            "Ep 2 (Step 000995): Train loss 5.569, Val loss 5.416\n",
            "Ep 2 (Step 001000): Train loss 5.311, Val loss 5.420\n",
            "Ep 2 (Step 001005): Train loss 5.093, Val loss 5.416\n",
            "Ep 2 (Step 001010): Train loss 5.518, Val loss 5.436\n",
            "Ep 2 (Step 001015): Train loss 5.009, Val loss 5.451\n",
            "Ep 2 (Step 001020): Train loss 4.832, Val loss 5.443\n",
            "Ep 2 (Step 001025): Train loss 5.148, Val loss 5.455\n",
            "Ep 2 (Step 001030): Train loss 4.866, Val loss 5.452\n",
            "Ep 2 (Step 001035): Train loss 5.316, Val loss 5.417\n",
            "Ep 2 (Step 001040): Train loss 5.432, Val loss 5.420\n",
            "Ep 2 (Step 001045): Train loss 4.775, Val loss 5.435\n",
            "Ep 2 (Step 001050): Train loss 5.278, Val loss 5.429\n",
            "Ep 2 (Step 001055): Train loss 4.831, Val loss 5.435\n",
            "Ep 2 (Step 001060): Train loss 5.415, Val loss 5.405\n",
            "Ep 2 (Step 001065): Train loss 4.891, Val loss 5.403\n",
            "Ep 2 (Step 001070): Train loss 5.258, Val loss 5.401\n",
            "Ep 2 (Step 001075): Train loss 5.379, Val loss 5.352\n",
            "Ep 2 (Step 001080): Train loss 4.653, Val loss 5.386\n",
            "Ep 2 (Step 001085): Train loss 5.304, Val loss 5.383\n",
            "Ep 2 (Step 001090): Train loss 5.045, Val loss 5.399\n",
            "Ep 2 (Step 001095): Train loss 4.900, Val loss 5.386\n",
            "Ep 2 (Step 001100): Train loss 5.780, Val loss 5.400\n",
            "Ep 2 (Step 001105): Train loss 4.624, Val loss 5.426\n",
            "Ep 2 (Step 001110): Train loss 5.553, Val loss 5.417\n",
            "Ep 2 (Step 001115): Train loss 5.165, Val loss 5.425\n",
            "Ep 2 (Step 001120): Train loss 4.943, Val loss 5.353\n",
            "Ep 2 (Step 001125): Train loss 5.085, Val loss 5.353\n",
            "Ep 2 (Step 001130): Train loss 4.789, Val loss 5.393\n",
            "Ep 2 (Step 001135): Train loss 5.634, Val loss 5.386\n",
            "Ep 2 (Step 001140): Train loss 4.621, Val loss 5.437\n",
            "Ep 2 (Step 001145): Train loss 4.888, Val loss 5.356\n",
            "Ep 2 (Step 001150): Train loss 4.912, Val loss 5.359\n",
            "Ep 2 (Step 001155): Train loss 4.922, Val loss 5.339\n",
            "Ep 2 (Step 001160): Train loss 5.338, Val loss 5.354\n",
            "Ep 2 (Step 001165): Train loss 5.214, Val loss 5.368\n",
            "Ep 2 (Step 001170): Train loss 4.879, Val loss 5.384\n",
            "Ep 2 (Step 001175): Train loss 4.915, Val loss 5.365\n",
            "Ep 2 (Step 001180): Train loss 4.734, Val loss 5.371\n",
            "Ep 2 (Step 001185): Train loss 4.812, Val loss 5.365\n",
            "Ep 2 (Step 001190): Train loss 5.238, Val loss 5.393\n",
            "Ep 2 (Step 001195): Train loss 5.047, Val loss 5.365\n",
            "Ep 2 (Step 001200): Train loss 5.341, Val loss 5.348\n",
            "Ep 2 (Step 001205): Train loss 5.085, Val loss 5.374\n",
            "Ep 2 (Step 001210): Train loss 4.659, Val loss 5.343\n",
            "Ep 2 (Step 001215): Train loss 5.035, Val loss 5.339\n",
            "Ep 2 (Step 001220): Train loss 4.779, Val loss 5.322\n",
            "Ep 2 (Step 001225): Train loss 4.919, Val loss 5.336\n",
            "Ep 2 (Step 001230): Train loss 5.973, Val loss 5.324\n",
            "Ep 2 (Step 001235): Train loss 5.193, Val loss 5.350\n",
            "Ep 2 (Step 001240): Train loss 5.132, Val loss 5.323\n",
            "Ep 2 (Step 001245): Train loss 5.483, Val loss 5.333\n",
            "Ep 2 (Step 001250): Train loss 5.152, Val loss 5.319\n",
            "Ep 2 (Step 001255): Train loss 4.489, Val loss 5.329\n",
            "Ep 2 (Step 001260): Train loss 5.147, Val loss 5.293\n",
            "Ep 2 (Step 001265): Train loss 5.049, Val loss 5.317\n",
            "Ep 2 (Step 001270): Train loss 5.051, Val loss 5.300\n",
            "Ep 2 (Step 001275): Train loss 5.019, Val loss 5.319\n",
            "Ep 2 (Step 001280): Train loss 5.253, Val loss 5.338\n",
            "Ep 2 (Step 001285): Train loss 4.757, Val loss 5.341\n",
            "Ep 2 (Step 001290): Train loss 5.069, Val loss 5.343\n",
            "Ep 2 (Step 001295): Train loss 5.224, Val loss 5.376\n",
            "Ep 2 (Step 001300): Train loss 5.527, Val loss 5.324\n",
            "Ep 2 (Step 001305): Train loss 4.610, Val loss 5.330\n",
            "Ep 2 (Step 001310): Train loss 5.095, Val loss 5.326\n",
            "Ep 2 (Step 001315): Train loss 5.116, Val loss 5.365\n",
            "Ep 2 (Step 001320): Train loss 5.323, Val loss 5.328\n",
            "Ep 2 (Step 001325): Train loss 5.000, Val loss 5.295\n",
            "Ep 2 (Step 001330): Train loss 4.669, Val loss 5.318\n",
            "Ep 2 (Step 001335): Train loss 5.100, Val loss 5.298\n",
            "Ep 2 (Step 001340): Train loss 5.119, Val loss 5.297\n",
            "Ep 2 (Step 001345): Train loss 4.552, Val loss 5.289\n",
            "Ep 2 (Step 001350): Train loss 4.902, Val loss 5.321\n",
            "Ep 2 (Step 001355): Train loss 5.898, Val loss 5.333\n",
            "Ep 2 (Step 001360): Train loss 4.831, Val loss 5.387\n",
            "Ep 2 (Step 001365): Train loss 4.893, Val loss 5.344\n",
            "Ep 2 (Step 001370): Train loss 4.570, Val loss 5.390\n",
            "Ep 2 (Step 001375): Train loss 5.122, Val loss 5.364\n",
            "Ep 2 (Step 001380): Train loss 5.217, Val loss 5.373\n",
            "Ep 2 (Step 001385): Train loss 4.832, Val loss 5.368\n",
            "Ep 2 (Step 001390): Train loss 5.402, Val loss 5.342\n",
            "Ep 2 (Step 001395): Train loss 5.373, Val loss 5.349\n",
            "Ep 2 (Step 001400): Train loss 5.036, Val loss 5.317\n",
            "Ep 2 (Step 001405): Train loss 5.094, Val loss 5.354\n",
            "Ep 2 (Step 001410): Train loss 5.046, Val loss 5.331\n",
            "Ep 2 (Step 001415): Train loss 4.540, Val loss 5.358\n",
            "Ep 2 (Step 001420): Train loss 5.628, Val loss 5.312\n",
            "Ep 2 (Step 001425): Train loss 5.668, Val loss 5.315\n",
            "Ep 2 (Step 001430): Train loss 5.386, Val loss 5.323\n",
            "Ep 2 (Step 001435): Train loss 5.183, Val loss 5.325\n",
            "Ep 2 (Step 001440): Train loss 4.669, Val loss 5.296\n",
            "Ep 2 (Step 001445): Train loss 4.752, Val loss 5.295\n",
            "Ep 2 (Step 001450): Train loss 5.278, Val loss 5.325\n",
            "Ep 2 (Step 001455): Train loss 5.479, Val loss 5.314\n",
            "Ep 2 (Step 001460): Train loss 5.242, Val loss 5.284\n",
            "Ep 2 (Step 001465): Train loss 5.278, Val loss 5.275\n",
            "Ep 2 (Step 001470): Train loss 4.694, Val loss 5.241\n",
            "Ep 2 (Step 001475): Train loss 5.253, Val loss 5.264\n",
            "Ep 2 (Step 001480): Train loss 4.914, Val loss 5.277\n",
            "Ep 2 (Step 001485): Train loss 4.515, Val loss 5.212\n",
            "Ep 2 (Step 001490): Train loss 5.486, Val loss 5.258\n",
            "Ep 2 (Step 001495): Train loss 4.585, Val loss 5.238\n",
            "Ep 2 (Step 001500): Train loss 5.361, Val loss 5.264\n",
            "Ep 2 (Step 001505): Train loss 5.151, Val loss 5.241\n",
            "Ep 2 (Step 001510): Train loss 6.075, Val loss 5.244\n",
            "Ep 2 (Step 001515): Train loss 5.104, Val loss 5.249\n",
            "Ep 2 (Step 001520): Train loss 5.199, Val loss 5.233\n",
            "Ep 2 (Step 001525): Train loss 4.747, Val loss 5.213\n",
            "Ep 2 (Step 001530): Train loss 4.503, Val loss 5.179\n",
            "Ep 2 (Step 001535): Train loss 5.354, Val loss 5.194\n",
            "Ep 2 (Step 001540): Train loss 4.354, Val loss 5.227\n",
            "Ep 2 (Step 001545): Train loss 5.277, Val loss 5.232\n",
            "Ep 2 (Step 001550): Train loss 4.855, Val loss 5.227\n",
            "Ep 2 (Step 001555): Train loss 5.061, Val loss 5.224\n",
            "Ep 2 (Step 001560): Train loss 4.733, Val loss 5.238\n",
            "Ep 2 (Step 001565): Train loss 4.979, Val loss 5.220\n",
            "Ep 2 (Step 001570): Train loss 4.996, Val loss 5.194\n",
            "Ep 2 (Step 001575): Train loss 5.296, Val loss 5.207\n",
            "Ep 2 (Step 001580): Train loss 5.232, Val loss 5.225\n",
            "Ep 2 (Step 001585): Train loss 4.925, Val loss 5.210\n",
            "Ep 2 (Step 001590): Train loss 4.493, Val loss 5.203\n",
            "Ep 2 (Step 001595): Train loss 4.564, Val loss 5.190\n",
            "Ep 2 (Step 001600): Train loss 5.091, Val loss 5.225\n",
            "Ep 2 (Step 001605): Train loss 4.941, Val loss 5.211\n",
            "Ep 2 (Step 001610): Train loss 4.869, Val loss 5.233\n",
            "Ep 2 (Step 001615): Train loss 4.888, Val loss 5.229\n",
            "Ep 2 (Step 001620): Train loss 4.532, Val loss 5.211\n",
            "Ep 2 (Step 001625): Train loss 4.991, Val loss 5.212\n",
            "Ep 2 (Step 001630): Train loss 5.072, Val loss 5.208\n",
            "Ep 2 (Step 001635): Train loss 4.856, Val loss 5.208\n",
            "Ep 2 (Step 001640): Train loss 4.542, Val loss 5.193\n",
            "Ep 2 (Step 001645): Train loss 5.202, Val loss 5.236\n",
            "Ep 2 (Step 001650): Train loss 4.801, Val loss 5.169\n",
            "Ep 2 (Step 001655): Train loss 4.847, Val loss 5.186\n",
            "Ep 2 (Step 001660): Train loss 5.060, Val loss 5.212\n",
            "Ep 2 (Step 001665): Train loss 4.829, Val loss 5.208\n",
            "Ep 2 (Step 001670): Train loss 4.896, Val loss 5.225\n",
            "Ep 2 (Step 001675): Train loss 4.843, Val loss 5.191\n",
            "Ep 2 (Step 001680): Train loss 4.935, Val loss 5.170\n",
            "Ep 2 (Step 001685): Train loss 4.793, Val loss 5.198\n",
            "Ep 2 (Step 001690): Train loss 5.146, Val loss 5.205\n",
            "Ep 2 (Step 001695): Train loss 5.029, Val loss 5.190\n",
            "Ep 2 (Step 001700): Train loss 4.835, Val loss 5.183\n",
            "Ep 2 (Step 001705): Train loss 5.315, Val loss 5.202\n",
            "Ep 2 (Step 001710): Train loss 4.916, Val loss 5.191\n",
            "Ep 2 (Step 001715): Train loss 5.563, Val loss 5.205\n",
            "Ep 2 (Step 001720): Train loss 5.254, Val loss 5.198\n",
            "Ep 2 (Step 001725): Train loss 5.625, Val loss 5.206\n",
            "Ep 2 (Step 001730): Train loss 4.402, Val loss 5.162\n",
            "Ep 2 (Step 001735): Train loss 5.113, Val loss 5.140\n",
            "Ep 2 (Step 001740): Train loss 4.973, Val loss 5.147\n",
            "Ep 2 (Step 001745): Train loss 4.269, Val loss 5.167\n",
            "Ep 2 (Step 001750): Train loss 4.922, Val loss 5.148\n",
            "Ep 2 (Step 001755): Train loss 4.550, Val loss 5.193\n",
            "Ep 2 (Step 001760): Train loss 5.284, Val loss 5.169\n",
            "Ep 2 (Step 001765): Train loss 4.607, Val loss 5.128\n",
            "Ep 2 (Step 001770): Train loss 5.027, Val loss 5.146\n",
            "Ep 2 (Step 001775): Train loss 5.034, Val loss 5.126\n",
            "Ep 2 (Step 001780): Train loss 4.374, Val loss 5.162\n",
            "Ep 2 (Step 001785): Train loss 5.066, Val loss 5.204\n",
            "Ep 2 (Step 001790): Train loss 5.334, Val loss 5.173\n",
            "Ep 2 (Step 001795): Train loss 5.102, Val loss 5.187\n",
            "Ep 2 (Step 001800): Train loss 4.482, Val loss 5.155\n",
            "Ep 2 (Step 001805): Train loss 5.047, Val loss 5.146\n",
            "Ep 2 (Step 001810): Train loss 5.083, Val loss 5.144\n",
            "Ep 2 (Step 001815): Train loss 5.269, Val loss 5.152\n",
            "He said we came here.  I am not be.  I am sure you know.   I am sure you know,I am not know you know you know you know you\n",
            "Ep 3 (Step 001820): Train loss 4.962, Val loss 5.153\n",
            "Ep 3 (Step 001825): Train loss 5.242, Val loss 5.139\n",
            "Ep 3 (Step 001830): Train loss 4.728, Val loss 5.147\n",
            "Ep 3 (Step 001835): Train loss 4.487, Val loss 5.197\n",
            "Ep 3 (Step 001840): Train loss 4.925, Val loss 5.140\n",
            "Ep 3 (Step 001845): Train loss 4.520, Val loss 5.151\n",
            "Ep 3 (Step 001850): Train loss 4.640, Val loss 5.193\n",
            "Ep 3 (Step 001855): Train loss 5.585, Val loss 5.186\n",
            "Ep 3 (Step 001860): Train loss 4.566, Val loss 5.106\n",
            "Ep 3 (Step 001865): Train loss 4.930, Val loss 5.125\n",
            "Ep 3 (Step 001870): Train loss 4.340, Val loss 5.156\n",
            "Ep 3 (Step 001875): Train loss 4.663, Val loss 5.157\n",
            "Ep 3 (Step 001880): Train loss 4.907, Val loss 5.138\n",
            "Ep 3 (Step 001885): Train loss 4.562, Val loss 5.155\n",
            "Ep 3 (Step 001890): Train loss 4.729, Val loss 5.123\n",
            "Ep 3 (Step 001895): Train loss 4.523, Val loss 5.133\n",
            "Ep 3 (Step 001900): Train loss 4.679, Val loss 5.121\n",
            "Ep 3 (Step 001905): Train loss 5.045, Val loss 5.188\n",
            "Ep 3 (Step 001910): Train loss 4.485, Val loss 5.181\n",
            "Ep 3 (Step 001915): Train loss 4.573, Val loss 5.118\n",
            "Ep 3 (Step 001920): Train loss 4.739, Val loss 5.115\n",
            "Ep 3 (Step 001925): Train loss 4.875, Val loss 5.148\n",
            "Ep 3 (Step 001930): Train loss 4.499, Val loss 5.135\n",
            "Ep 3 (Step 001935): Train loss 4.692, Val loss 5.140\n",
            "Ep 3 (Step 001940): Train loss 4.898, Val loss 5.126\n",
            "Ep 3 (Step 001945): Train loss 4.583, Val loss 5.170\n",
            "Ep 3 (Step 001950): Train loss 4.674, Val loss 5.143\n",
            "Ep 3 (Step 001955): Train loss 4.412, Val loss 5.140\n",
            "Ep 3 (Step 001960): Train loss 4.982, Val loss 5.170\n",
            "Ep 3 (Step 001965): Train loss 4.313, Val loss 5.158\n",
            "Ep 3 (Step 001970): Train loss 4.829, Val loss 5.228\n",
            "Ep 3 (Step 001975): Train loss 4.963, Val loss 5.169\n",
            "Ep 3 (Step 001980): Train loss 4.605, Val loss 5.179\n",
            "Ep 3 (Step 001985): Train loss 4.974, Val loss 5.138\n",
            "Ep 3 (Step 001990): Train loss 5.031, Val loss 5.134\n",
            "Ep 3 (Step 001995): Train loss 4.393, Val loss 5.137\n",
            "Ep 3 (Step 002000): Train loss 4.445, Val loss 5.139\n",
            "Ep 3 (Step 002005): Train loss 5.070, Val loss 5.153\n",
            "Ep 3 (Step 002010): Train loss 4.140, Val loss 5.107\n",
            "Ep 3 (Step 002015): Train loss 4.925, Val loss 5.107\n",
            "Ep 3 (Step 002020): Train loss 4.743, Val loss 5.113\n",
            "Ep 3 (Step 002025): Train loss 4.592, Val loss 5.120\n",
            "Ep 3 (Step 002030): Train loss 4.456, Val loss 5.120\n",
            "Ep 3 (Step 002035): Train loss 5.205, Val loss 5.145\n",
            "Ep 3 (Step 002040): Train loss 4.804, Val loss 5.130\n",
            "Ep 3 (Step 002045): Train loss 4.720, Val loss 5.111\n",
            "Ep 3 (Step 002050): Train loss 4.820, Val loss 5.137\n",
            "Ep 3 (Step 002055): Train loss 5.021, Val loss 5.087\n",
            "Ep 3 (Step 002060): Train loss 4.798, Val loss 5.086\n",
            "Ep 3 (Step 002065): Train loss 4.516, Val loss 5.110\n",
            "Ep 3 (Step 002070): Train loss 4.389, Val loss 5.114\n",
            "Ep 3 (Step 002075): Train loss 4.354, Val loss 5.068\n",
            "Ep 3 (Step 002080): Train loss 4.502, Val loss 5.094\n",
            "Ep 3 (Step 002085): Train loss 4.580, Val loss 5.074\n",
            "Ep 3 (Step 002090): Train loss 4.275, Val loss 5.085\n",
            "Ep 3 (Step 002095): Train loss 5.006, Val loss 5.094\n",
            "Ep 3 (Step 002100): Train loss 4.158, Val loss 5.133\n",
            "Ep 3 (Step 002105): Train loss 4.998, Val loss 5.099\n",
            "Ep 3 (Step 002110): Train loss 4.580, Val loss 5.072\n",
            "Ep 3 (Step 002115): Train loss 4.402, Val loss 5.099\n",
            "Ep 3 (Step 002120): Train loss 4.853, Val loss 5.104\n",
            "Ep 3 (Step 002125): Train loss 4.717, Val loss 5.110\n",
            "Ep 3 (Step 002130): Train loss 4.587, Val loss 5.044\n",
            "Ep 3 (Step 002135): Train loss 4.867, Val loss 5.043\n",
            "Ep 3 (Step 002140): Train loss 4.716, Val loss 5.047\n",
            "Ep 3 (Step 002145): Train loss 5.095, Val loss 5.050\n",
            "Ep 3 (Step 002150): Train loss 4.409, Val loss 5.041\n",
            "Ep 3 (Step 002155): Train loss 4.841, Val loss 5.050\n",
            "Ep 3 (Step 002160): Train loss 4.573, Val loss 5.039\n",
            "Ep 3 (Step 002165): Train loss 4.543, Val loss 5.104\n",
            "Ep 3 (Step 002170): Train loss 4.973, Val loss 5.110\n",
            "Ep 3 (Step 002175): Train loss 5.274, Val loss 5.061\n",
            "Ep 3 (Step 002180): Train loss 4.629, Val loss 5.116\n",
            "Ep 3 (Step 002185): Train loss 4.836, Val loss 5.068\n",
            "Ep 3 (Step 002190): Train loss 5.525, Val loss 5.088\n",
            "Ep 3 (Step 002195): Train loss 4.745, Val loss 5.097\n",
            "Ep 3 (Step 002200): Train loss 4.978, Val loss 5.053\n",
            "Ep 3 (Step 002205): Train loss 4.444, Val loss 5.040\n",
            "Ep 3 (Step 002210): Train loss 5.383, Val loss 5.066\n",
            "Ep 3 (Step 002215): Train loss 4.389, Val loss 5.041\n",
            "Ep 3 (Step 002220): Train loss 5.475, Val loss 5.057\n",
            "Ep 3 (Step 002225): Train loss 4.528, Val loss 5.048\n",
            "Ep 3 (Step 002230): Train loss 4.484, Val loss 5.078\n",
            "Ep 3 (Step 002235): Train loss 4.838, Val loss 5.051\n",
            "Ep 3 (Step 002240): Train loss 4.973, Val loss 5.076\n",
            "Ep 3 (Step 002245): Train loss 4.466, Val loss 5.072\n",
            "Ep 3 (Step 002250): Train loss 5.013, Val loss 5.079\n",
            "Ep 3 (Step 002255): Train loss 4.959, Val loss 5.016\n",
            "Ep 3 (Step 002260): Train loss 4.575, Val loss 5.023\n",
            "Ep 3 (Step 002265): Train loss 4.540, Val loss 5.025\n",
            "Ep 3 (Step 002270): Train loss 4.449, Val loss 5.061\n",
            "Ep 3 (Step 002275): Train loss 4.831, Val loss 5.112\n",
            "Ep 3 (Step 002280): Train loss 4.785, Val loss 5.100\n",
            "Ep 3 (Step 002285): Train loss 4.562, Val loss 5.057\n",
            "Ep 3 (Step 002290): Train loss 4.774, Val loss 5.047\n",
            "Ep 3 (Step 002295): Train loss 4.817, Val loss 5.046\n",
            "Ep 3 (Step 002300): Train loss 4.763, Val loss 5.123\n",
            "Ep 3 (Step 002305): Train loss 4.769, Val loss 5.044\n",
            "Ep 3 (Step 002310): Train loss 4.531, Val loss 5.105\n",
            "Ep 3 (Step 002315): Train loss 4.637, Val loss 5.016\n",
            "Ep 3 (Step 002320): Train loss 4.809, Val loss 4.998\n",
            "Ep 3 (Step 002325): Train loss 5.072, Val loss 5.016\n",
            "Ep 3 (Step 002330): Train loss 4.196, Val loss 5.016\n",
            "Ep 3 (Step 002335): Train loss 5.257, Val loss 5.055\n",
            "Ep 3 (Step 002340): Train loss 4.786, Val loss 5.078\n",
            "Ep 3 (Step 002345): Train loss 5.166, Val loss 5.041\n",
            "Ep 3 (Step 002350): Train loss 5.107, Val loss 5.009\n",
            "Ep 3 (Step 002355): Train loss 4.795, Val loss 4.990\n",
            "Ep 3 (Step 002360): Train loss 4.587, Val loss 5.011\n",
            "Ep 3 (Step 002365): Train loss 4.400, Val loss 5.036\n",
            "Ep 3 (Step 002370): Train loss 4.351, Val loss 5.011\n",
            "Ep 3 (Step 002375): Train loss 4.749, Val loss 4.993\n",
            "Ep 3 (Step 002380): Train loss 4.627, Val loss 4.971\n",
            "Ep 3 (Step 002385): Train loss 4.713, Val loss 5.011\n",
            "Ep 3 (Step 002390): Train loss 4.728, Val loss 5.034\n",
            "Ep 3 (Step 002395): Train loss 4.102, Val loss 5.012\n",
            "Ep 3 (Step 002400): Train loss 4.792, Val loss 5.009\n",
            "Ep 3 (Step 002405): Train loss 4.669, Val loss 5.019\n",
            "Ep 3 (Step 002410): Train loss 4.735, Val loss 5.032\n",
            "Ep 3 (Step 002415): Train loss 4.451, Val loss 5.009\n",
            "Ep 3 (Step 002420): Train loss 4.720, Val loss 4.995\n",
            "Ep 3 (Step 002425): Train loss 5.074, Val loss 4.975\n",
            "Ep 3 (Step 002430): Train loss 4.575, Val loss 4.963\n",
            "Ep 3 (Step 002435): Train loss 4.427, Val loss 4.975\n",
            "Ep 3 (Step 002440): Train loss 4.540, Val loss 4.960\n",
            "Ep 3 (Step 002445): Train loss 4.709, Val loss 5.001\n",
            "Ep 3 (Step 002450): Train loss 4.988, Val loss 4.995\n",
            "Ep 3 (Step 002455): Train loss 4.777, Val loss 4.979\n",
            "Ep 3 (Step 002460): Train loss 4.122, Val loss 4.988\n",
            "Ep 3 (Step 002465): Train loss 4.570, Val loss 4.973\n",
            "Ep 3 (Step 002470): Train loss 4.538, Val loss 4.966\n",
            "Ep 3 (Step 002475): Train loss 4.953, Val loss 4.978\n",
            "Ep 3 (Step 002480): Train loss 4.118, Val loss 4.986\n",
            "Ep 3 (Step 002485): Train loss 4.517, Val loss 4.951\n",
            "Ep 3 (Step 002490): Train loss 4.203, Val loss 5.030\n",
            "Ep 3 (Step 002495): Train loss 4.304, Val loss 4.988\n",
            "Ep 3 (Step 002500): Train loss 4.630, Val loss 5.008\n",
            "Ep 3 (Step 002505): Train loss 4.688, Val loss 4.992\n",
            "Ep 3 (Step 002510): Train loss 4.642, Val loss 5.020\n",
            "Ep 3 (Step 002515): Train loss 4.767, Val loss 5.009\n",
            "Ep 3 (Step 002520): Train loss 4.859, Val loss 4.963\n",
            "Ep 3 (Step 002525): Train loss 4.638, Val loss 4.933\n",
            "Ep 3 (Step 002530): Train loss 4.992, Val loss 4.910\n",
            "Ep 3 (Step 002535): Train loss 3.839, Val loss 4.930\n",
            "Ep 3 (Step 002540): Train loss 4.449, Val loss 4.940\n",
            "Ep 3 (Step 002545): Train loss 4.465, Val loss 4.985\n",
            "Ep 3 (Step 002550): Train loss 4.552, Val loss 4.975\n",
            "Ep 3 (Step 002555): Train loss 4.040, Val loss 4.987\n",
            "Ep 3 (Step 002560): Train loss 4.449, Val loss 4.989\n",
            "Ep 3 (Step 002565): Train loss 4.703, Val loss 4.961\n",
            "Ep 3 (Step 002570): Train loss 4.179, Val loss 4.948\n",
            "Ep 3 (Step 002575): Train loss 4.299, Val loss 5.000\n",
            "Ep 3 (Step 002580): Train loss 4.371, Val loss 4.940\n",
            "Ep 3 (Step 002585): Train loss 4.027, Val loss 4.963\n",
            "Ep 3 (Step 002590): Train loss 4.510, Val loss 4.955\n",
            "Ep 3 (Step 002595): Train loss 4.729, Val loss 4.916\n",
            "Ep 3 (Step 002600): Train loss 4.511, Val loss 4.923\n",
            "Ep 3 (Step 002605): Train loss 4.849, Val loss 4.886\n",
            "Ep 3 (Step 002610): Train loss 4.884, Val loss 4.892\n",
            "Ep 3 (Step 002615): Train loss 4.614, Val loss 4.910\n",
            "Ep 3 (Step 002620): Train loss 4.065, Val loss 4.901\n",
            "Ep 3 (Step 002625): Train loss 4.566, Val loss 4.942\n",
            "Ep 3 (Step 002630): Train loss 4.996, Val loss 4.933\n",
            "Ep 3 (Step 002635): Train loss 4.189, Val loss 4.884\n",
            "Ep 3 (Step 002640): Train loss 4.522, Val loss 4.923\n",
            "Ep 3 (Step 002645): Train loss 4.359, Val loss 4.879\n",
            "Ep 3 (Step 002650): Train loss 4.439, Val loss 4.945\n",
            "Ep 3 (Step 002655): Train loss 4.336, Val loss 4.899\n",
            "Ep 3 (Step 002660): Train loss 4.815, Val loss 4.930\n",
            "Ep 3 (Step 002665): Train loss 4.441, Val loss 4.870\n",
            "Ep 3 (Step 002670): Train loss 4.507, Val loss 4.880\n",
            "Ep 3 (Step 002675): Train loss 5.101, Val loss 4.871\n",
            "Ep 3 (Step 002680): Train loss 4.435, Val loss 4.926\n",
            "Ep 3 (Step 002685): Train loss 4.551, Val loss 4.914\n",
            "Ep 3 (Step 002690): Train loss 4.262, Val loss 4.881\n",
            "Ep 3 (Step 002695): Train loss 4.762, Val loss 4.883\n",
            "Ep 3 (Step 002700): Train loss 4.285, Val loss 4.902\n",
            "Ep 3 (Step 002705): Train loss 4.317, Val loss 4.897\n",
            "Ep 3 (Step 002710): Train loss 4.668, Val loss 4.892\n",
            "Ep 3 (Step 002715): Train loss 4.572, Val loss 4.863\n",
            "Ep 3 (Step 002720): Train loss 4.112, Val loss 4.852\n",
            "He said we came here.  I have been to be a man, and the time, and the same time of the the time, and the same time, and the same time, and the the whole of the room. \n",
            "Ep 4 (Step 002725): Train loss 4.740, Val loss 4.890\n",
            "Ep 4 (Step 002730): Train loss 4.132, Val loss 4.851\n",
            "Ep 4 (Step 002735): Train loss 4.730, Val loss 4.911\n",
            "Ep 4 (Step 002740): Train loss 4.794, Val loss 4.888\n",
            "Ep 4 (Step 002745): Train loss 4.382, Val loss 4.888\n",
            "Ep 4 (Step 002750): Train loss 3.962, Val loss 4.874\n",
            "Ep 4 (Step 002755): Train loss 4.510, Val loss 4.862\n",
            "Ep 4 (Step 002760): Train loss 4.439, Val loss 4.863\n",
            "Ep 4 (Step 002765): Train loss 3.995, Val loss 4.881\n",
            "Ep 4 (Step 002770): Train loss 4.632, Val loss 4.898\n",
            "Ep 4 (Step 002775): Train loss 4.610, Val loss 4.887\n",
            "Ep 4 (Step 002780): Train loss 4.689, Val loss 4.872\n",
            "Ep 4 (Step 002785): Train loss 4.350, Val loss 4.850\n",
            "Ep 4 (Step 002790): Train loss 4.416, Val loss 4.830\n",
            "Ep 4 (Step 002795): Train loss 4.373, Val loss 4.839\n",
            "Ep 4 (Step 002800): Train loss 4.690, Val loss 4.856\n",
            "Ep 4 (Step 002805): Train loss 4.366, Val loss 4.861\n",
            "Ep 4 (Step 002810): Train loss 4.443, Val loss 4.846\n",
            "Ep 4 (Step 002815): Train loss 4.659, Val loss 4.864\n",
            "Ep 4 (Step 002820): Train loss 4.402, Val loss 4.840\n",
            "Ep 4 (Step 002825): Train loss 4.088, Val loss 4.874\n",
            "Ep 4 (Step 002830): Train loss 5.048, Val loss 4.853\n",
            "Ep 4 (Step 002835): Train loss 4.497, Val loss 4.856\n",
            "Ep 4 (Step 002840): Train loss 4.449, Val loss 4.861\n",
            "Ep 4 (Step 002845): Train loss 4.303, Val loss 4.839\n",
            "Ep 4 (Step 002850): Train loss 4.302, Val loss 4.832\n",
            "Ep 4 (Step 002855): Train loss 4.356, Val loss 4.856\n",
            "Ep 4 (Step 002860): Train loss 4.239, Val loss 4.896\n",
            "Ep 4 (Step 002865): Train loss 3.822, Val loss 4.868\n",
            "Ep 4 (Step 002870): Train loss 4.165, Val loss 4.895\n",
            "Ep 4 (Step 002875): Train loss 4.510, Val loss 4.842\n",
            "Ep 4 (Step 002880): Train loss 4.249, Val loss 4.845\n",
            "Ep 4 (Step 002885): Train loss 4.543, Val loss 4.842\n",
            "Ep 4 (Step 002890): Train loss 4.391, Val loss 4.903\n",
            "Ep 4 (Step 002895): Train loss 4.857, Val loss 4.842\n",
            "Ep 4 (Step 002900): Train loss 4.550, Val loss 4.888\n",
            "Ep 4 (Step 002905): Train loss 4.655, Val loss 4.910\n",
            "Ep 4 (Step 002910): Train loss 4.494, Val loss 4.906\n",
            "Ep 4 (Step 002915): Train loss 4.109, Val loss 4.916\n",
            "Ep 4 (Step 002920): Train loss 4.713, Val loss 4.901\n",
            "Ep 4 (Step 002925): Train loss 4.395, Val loss 4.847\n",
            "Ep 4 (Step 002930): Train loss 4.595, Val loss 4.830\n",
            "Ep 4 (Step 002935): Train loss 4.033, Val loss 4.850\n",
            "Ep 4 (Step 002940): Train loss 4.365, Val loss 4.842\n",
            "Ep 4 (Step 002945): Train loss 4.384, Val loss 4.825\n",
            "Ep 4 (Step 002950): Train loss 4.025, Val loss 4.826\n",
            "Ep 4 (Step 002955): Train loss 4.627, Val loss 4.835\n",
            "Ep 4 (Step 002960): Train loss 4.573, Val loss 4.829\n",
            "Ep 4 (Step 002965): Train loss 4.324, Val loss 4.847\n",
            "Ep 4 (Step 002970): Train loss 4.218, Val loss 4.892\n",
            "Ep 4 (Step 002975): Train loss 4.165, Val loss 4.890\n",
            "Ep 4 (Step 002980): Train loss 4.208, Val loss 4.856\n",
            "Ep 4 (Step 002985): Train loss 4.520, Val loss 4.845\n",
            "Ep 4 (Step 002990): Train loss 4.535, Val loss 4.854\n",
            "Ep 4 (Step 002995): Train loss 4.268, Val loss 4.830\n",
            "Ep 4 (Step 003000): Train loss 4.465, Val loss 4.833\n",
            "Ep 4 (Step 003005): Train loss 4.804, Val loss 4.863\n",
            "Ep 4 (Step 003010): Train loss 4.342, Val loss 4.812\n",
            "Ep 4 (Step 003015): Train loss 4.855, Val loss 4.803\n",
            "Ep 4 (Step 003020): Train loss 3.965, Val loss 4.804\n",
            "Ep 4 (Step 003025): Train loss 4.600, Val loss 4.807\n",
            "Ep 4 (Step 003030): Train loss 4.452, Val loss 4.791\n",
            "Ep 4 (Step 003035): Train loss 4.250, Val loss 4.832\n",
            "Ep 4 (Step 003040): Train loss 4.511, Val loss 4.831\n",
            "Ep 4 (Step 003045): Train loss 4.327, Val loss 4.846\n",
            "Ep 4 (Step 003050): Train loss 4.365, Val loss 4.819\n",
            "Ep 4 (Step 003055): Train loss 4.365, Val loss 4.850\n",
            "Ep 4 (Step 003060): Train loss 3.867, Val loss 4.851\n",
            "Ep 4 (Step 003065): Train loss 4.978, Val loss 4.816\n",
            "Ep 4 (Step 003070): Train loss 4.548, Val loss 4.801\n",
            "Ep 4 (Step 003075): Train loss 4.659, Val loss 4.839\n",
            "Ep 4 (Step 003080): Train loss 4.501, Val loss 4.833\n",
            "Ep 4 (Step 003085): Train loss 4.332, Val loss 4.809\n",
            "Ep 4 (Step 003090): Train loss 4.215, Val loss 4.852\n",
            "Ep 4 (Step 003095): Train loss 4.219, Val loss 4.851\n",
            "Ep 4 (Step 003100): Train loss 3.906, Val loss 4.802\n",
            "Ep 4 (Step 003105): Train loss 4.479, Val loss 4.805\n",
            "Ep 4 (Step 003110): Train loss 4.228, Val loss 4.835\n",
            "Ep 4 (Step 003115): Train loss 4.572, Val loss 4.818\n",
            "Ep 4 (Step 003120): Train loss 4.247, Val loss 4.802\n",
            "Ep 4 (Step 003125): Train loss 4.085, Val loss 4.795\n",
            "Ep 4 (Step 003130): Train loss 4.434, Val loss 4.798\n",
            "Ep 4 (Step 003135): Train loss 4.556, Val loss 4.823\n",
            "Ep 4 (Step 003140): Train loss 4.347, Val loss 4.823\n",
            "Ep 4 (Step 003145): Train loss 4.191, Val loss 4.831\n",
            "Ep 4 (Step 003150): Train loss 4.470, Val loss 4.812\n",
            "Ep 4 (Step 003155): Train loss 4.288, Val loss 4.852\n",
            "Ep 4 (Step 003160): Train loss 4.079, Val loss 4.768\n",
            "Ep 4 (Step 003165): Train loss 4.230, Val loss 4.822\n",
            "Ep 4 (Step 003170): Train loss 4.552, Val loss 4.787\n",
            "Ep 4 (Step 003175): Train loss 4.737, Val loss 4.814\n",
            "Ep 4 (Step 003180): Train loss 4.690, Val loss 4.783\n",
            "Ep 4 (Step 003185): Train loss 4.642, Val loss 4.815\n",
            "Ep 4 (Step 003190): Train loss 4.005, Val loss 4.809\n",
            "Ep 4 (Step 003195): Train loss 4.222, Val loss 4.793\n",
            "Ep 4 (Step 003200): Train loss 4.374, Val loss 4.796\n",
            "Ep 4 (Step 003205): Train loss 4.232, Val loss 4.757\n",
            "Ep 4 (Step 003210): Train loss 4.158, Val loss 4.824\n",
            "Ep 4 (Step 003215): Train loss 4.672, Val loss 4.813\n",
            "Ep 4 (Step 003220): Train loss 4.497, Val loss 4.803\n",
            "Ep 4 (Step 003225): Train loss 3.891, Val loss 4.790\n",
            "Ep 4 (Step 003230): Train loss 4.924, Val loss 4.836\n",
            "Ep 4 (Step 003235): Train loss 3.899, Val loss 4.814\n",
            "Ep 4 (Step 003240): Train loss 3.953, Val loss 4.791\n",
            "Ep 4 (Step 003245): Train loss 4.295, Val loss 4.793\n",
            "Ep 4 (Step 003250): Train loss 4.338, Val loss 4.808\n",
            "Ep 4 (Step 003255): Train loss 4.648, Val loss 4.804\n",
            "Ep 4 (Step 003260): Train loss 4.057, Val loss 4.819\n",
            "Ep 4 (Step 003265): Train loss 4.492, Val loss 4.818\n",
            "Ep 4 (Step 003270): Train loss 4.097, Val loss 4.815\n",
            "Ep 4 (Step 003275): Train loss 4.248, Val loss 4.855\n",
            "Ep 4 (Step 003280): Train loss 4.678, Val loss 4.857\n",
            "Ep 4 (Step 003285): Train loss 4.764, Val loss 4.814\n",
            "Ep 4 (Step 003290): Train loss 4.218, Val loss 4.825\n",
            "Ep 4 (Step 003295): Train loss 4.415, Val loss 4.835\n",
            "Ep 4 (Step 003300): Train loss 4.240, Val loss 4.820\n",
            "Ep 4 (Step 003305): Train loss 4.211, Val loss 4.797\n",
            "Ep 4 (Step 003310): Train loss 4.415, Val loss 4.755\n",
            "Ep 4 (Step 003315): Train loss 3.829, Val loss 4.768\n",
            "Ep 4 (Step 003320): Train loss 4.322, Val loss 4.751\n",
            "Ep 4 (Step 003325): Train loss 4.457, Val loss 4.785\n",
            "Ep 4 (Step 003330): Train loss 4.064, Val loss 4.780\n",
            "Ep 4 (Step 003335): Train loss 4.083, Val loss 4.777\n",
            "Ep 4 (Step 003340): Train loss 4.554, Val loss 4.756\n",
            "Ep 4 (Step 003345): Train loss 4.224, Val loss 4.776\n",
            "Ep 4 (Step 003350): Train loss 3.999, Val loss 4.780\n",
            "Ep 4 (Step 003355): Train loss 4.449, Val loss 4.770\n",
            "Ep 4 (Step 003360): Train loss 4.344, Val loss 4.768\n",
            "Ep 4 (Step 003365): Train loss 4.490, Val loss 4.790\n",
            "Ep 4 (Step 003370): Train loss 4.138, Val loss 4.774\n",
            "Ep 4 (Step 003375): Train loss 4.059, Val loss 4.775\n",
            "Ep 4 (Step 003380): Train loss 4.307, Val loss 4.759\n",
            "Ep 4 (Step 003385): Train loss 3.925, Val loss 4.768\n",
            "Ep 4 (Step 003390): Train loss 4.376, Val loss 4.772\n",
            "Ep 4 (Step 003395): Train loss 4.270, Val loss 4.777\n",
            "Ep 4 (Step 003400): Train loss 3.700, Val loss 4.757\n",
            "Ep 4 (Step 003405): Train loss 4.263, Val loss 4.736\n",
            "Ep 4 (Step 003410): Train loss 4.168, Val loss 4.785\n",
            "Ep 4 (Step 003415): Train loss 4.654, Val loss 4.758\n",
            "Ep 4 (Step 003420): Train loss 4.437, Val loss 4.793\n",
            "Ep 4 (Step 003425): Train loss 4.343, Val loss 4.772\n",
            "Ep 4 (Step 003430): Train loss 4.496, Val loss 4.758\n",
            "Ep 4 (Step 003435): Train loss 4.188, Val loss 4.753\n",
            "Ep 4 (Step 003440): Train loss 4.989, Val loss 4.752\n",
            "Ep 4 (Step 003445): Train loss 4.629, Val loss 4.743\n",
            "Ep 4 (Step 003450): Train loss 4.500, Val loss 4.752\n",
            "Ep 4 (Step 003455): Train loss 4.439, Val loss 4.755\n",
            "Ep 4 (Step 003460): Train loss 3.966, Val loss 4.727\n",
            "Ep 4 (Step 003465): Train loss 4.049, Val loss 4.719\n",
            "Ep 4 (Step 003470): Train loss 4.374, Val loss 4.715\n",
            "Ep 4 (Step 003475): Train loss 4.528, Val loss 4.740\n",
            "Ep 4 (Step 003480): Train loss 4.011, Val loss 4.718\n",
            "Ep 4 (Step 003485): Train loss 4.212, Val loss 4.709\n",
            "Ep 4 (Step 003490): Train loss 4.040, Val loss 4.737\n",
            "Ep 4 (Step 003495): Train loss 3.955, Val loss 4.743\n",
            "Ep 4 (Step 003500): Train loss 4.244, Val loss 4.767\n",
            "Ep 4 (Step 003505): Train loss 4.115, Val loss 4.770\n",
            "Ep 4 (Step 003510): Train loss 4.228, Val loss 4.740\n",
            "Ep 4 (Step 003515): Train loss 4.594, Val loss 4.730\n",
            "Ep 4 (Step 003520): Train loss 4.374, Val loss 4.735\n",
            "Ep 4 (Step 003525): Train loss 3.968, Val loss 4.714\n",
            "Ep 4 (Step 003530): Train loss 4.147, Val loss 4.724\n",
            "Ep 4 (Step 003535): Train loss 3.925, Val loss 4.705\n",
            "Ep 4 (Step 003540): Train loss 4.561, Val loss 4.702\n",
            "Ep 4 (Step 003545): Train loss 4.362, Val loss 4.734\n",
            "Ep 4 (Step 003550): Train loss 4.473, Val loss 4.722\n",
            "Ep 4 (Step 003555): Train loss 3.927, Val loss 4.745\n",
            "Ep 4 (Step 003560): Train loss 4.753, Val loss 4.753\n",
            "Ep 4 (Step 003565): Train loss 4.223, Val loss 4.766\n",
            "Ep 4 (Step 003570): Train loss 4.254, Val loss 4.721\n",
            "Ep 4 (Step 003575): Train loss 4.414, Val loss 4.742\n",
            "Ep 4 (Step 003580): Train loss 3.996, Val loss 4.752\n",
            "Ep 4 (Step 003585): Train loss 4.444, Val loss 4.716\n",
            "Ep 4 (Step 003590): Train loss 4.399, Val loss 4.730\n",
            "Ep 4 (Step 003595): Train loss 4.421, Val loss 4.692\n",
            "Ep 4 (Step 003600): Train loss 4.588, Val loss 4.711\n",
            "Ep 4 (Step 003605): Train loss 4.133, Val loss 4.690\n",
            "Ep 4 (Step 003610): Train loss 4.108, Val loss 4.713\n",
            "Ep 4 (Step 003615): Train loss 4.251, Val loss 4.682\n",
            "Ep 4 (Step 003620): Train loss 4.167, Val loss 4.674\n",
            "Ep 4 (Step 003625): Train loss 4.443, Val loss 4.702\n",
            "Ep 4 (Step 003630): Train loss 4.691, Val loss 4.715\n",
            "He said we came here.  I don he said Lucy, I do you know what I do you know what I have and I am not know that I am not know that I am sure, and I am not \n",
            "Ep 5 (Step 003635): Train loss 4.484, Val loss 4.710\n",
            "Ep 5 (Step 003640): Train loss 3.845, Val loss 4.705\n",
            "Ep 5 (Step 003645): Train loss 4.038, Val loss 4.692\n",
            "Ep 5 (Step 003650): Train loss 4.251, Val loss 4.700\n",
            "Ep 5 (Step 003655): Train loss 4.149, Val loss 4.711\n",
            "Ep 5 (Step 003660): Train loss 4.413, Val loss 4.682\n",
            "Ep 5 (Step 003665): Train loss 4.398, Val loss 4.708\n",
            "Ep 5 (Step 003670): Train loss 4.466, Val loss 4.695\n",
            "Ep 5 (Step 003675): Train loss 4.011, Val loss 4.703\n",
            "Ep 5 (Step 003680): Train loss 3.686, Val loss 4.664\n",
            "Ep 5 (Step 003685): Train loss 3.896, Val loss 4.680\n",
            "Ep 5 (Step 003690): Train loss 3.718, Val loss 4.651\n",
            "Ep 5 (Step 003695): Train loss 4.251, Val loss 4.684\n",
            "Ep 5 (Step 003700): Train loss 4.451, Val loss 4.710\n",
            "Ep 5 (Step 003705): Train loss 4.748, Val loss 4.693\n",
            "Ep 5 (Step 003710): Train loss 4.155, Val loss 4.674\n",
            "Ep 5 (Step 003715): Train loss 3.923, Val loss 4.668\n",
            "Ep 5 (Step 003720): Train loss 4.051, Val loss 4.667\n",
            "Ep 5 (Step 003725): Train loss 4.303, Val loss 4.669\n",
            "Ep 5 (Step 003730): Train loss 3.647, Val loss 4.643\n",
            "Ep 5 (Step 003735): Train loss 3.802, Val loss 4.654\n",
            "Ep 5 (Step 003740): Train loss 4.244, Val loss 4.681\n",
            "Ep 5 (Step 003745): Train loss 3.829, Val loss 4.715\n",
            "Ep 5 (Step 003750): Train loss 4.426, Val loss 4.652\n",
            "Ep 5 (Step 003755): Train loss 4.279, Val loss 4.648\n",
            "Ep 5 (Step 003760): Train loss 4.378, Val loss 4.647\n",
            "Ep 5 (Step 003765): Train loss 4.138, Val loss 4.648\n",
            "Ep 5 (Step 003770): Train loss 4.797, Val loss 4.688\n",
            "Ep 5 (Step 003775): Train loss 3.948, Val loss 4.661\n",
            "Ep 5 (Step 003780): Train loss 4.117, Val loss 4.711\n",
            "Ep 5 (Step 003785): Train loss 4.389, Val loss 4.678\n",
            "Ep 5 (Step 003790): Train loss 3.930, Val loss 4.660\n",
            "Ep 5 (Step 003795): Train loss 4.243, Val loss 4.666\n",
            "Ep 5 (Step 003800): Train loss 4.171, Val loss 4.677\n",
            "Ep 5 (Step 003805): Train loss 4.050, Val loss 4.668\n",
            "Ep 5 (Step 003810): Train loss 4.735, Val loss 4.648\n",
            "Ep 5 (Step 003815): Train loss 3.442, Val loss 4.645\n",
            "Ep 5 (Step 003820): Train loss 4.328, Val loss 4.697\n",
            "Ep 5 (Step 003825): Train loss 4.340, Val loss 4.684\n",
            "Ep 5 (Step 003830): Train loss 3.996, Val loss 4.664\n",
            "Ep 5 (Step 003835): Train loss 4.041, Val loss 4.701\n",
            "Ep 5 (Step 003840): Train loss 4.216, Val loss 4.698\n",
            "Ep 5 (Step 003845): Train loss 4.035, Val loss 4.686\n",
            "Ep 5 (Step 003850): Train loss 4.044, Val loss 4.705\n",
            "Ep 5 (Step 003855): Train loss 4.233, Val loss 4.696\n",
            "Ep 5 (Step 003860): Train loss 3.458, Val loss 4.718\n",
            "Ep 5 (Step 003865): Train loss 4.762, Val loss 4.724\n",
            "Ep 5 (Step 003870): Train loss 4.565, Val loss 4.712\n",
            "Ep 5 (Step 003875): Train loss 4.062, Val loss 4.679\n",
            "Ep 5 (Step 003880): Train loss 4.338, Val loss 4.718\n",
            "Ep 5 (Step 003885): Train loss 4.612, Val loss 4.675\n",
            "Ep 5 (Step 003890): Train loss 4.167, Val loss 4.689\n",
            "Ep 5 (Step 003895): Train loss 4.072, Val loss 4.680\n",
            "Ep 5 (Step 003900): Train loss 4.122, Val loss 4.693\n",
            "Ep 5 (Step 003905): Train loss 4.611, Val loss 4.682\n",
            "Ep 5 (Step 003910): Train loss 4.255, Val loss 4.695\n",
            "Ep 5 (Step 003915): Train loss 4.333, Val loss 4.720\n",
            "Ep 5 (Step 003920): Train loss 4.246, Val loss 4.706\n",
            "Ep 5 (Step 003925): Train loss 4.061, Val loss 4.695\n",
            "Ep 5 (Step 003930): Train loss 4.118, Val loss 4.703\n",
            "Ep 5 (Step 003935): Train loss 4.223, Val loss 4.694\n",
            "Ep 5 (Step 003940): Train loss 3.966, Val loss 4.704\n",
            "Ep 5 (Step 003945): Train loss 4.284, Val loss 4.674\n",
            "Ep 5 (Step 003950): Train loss 3.863, Val loss 4.688\n",
            "Ep 5 (Step 003955): Train loss 4.250, Val loss 4.665\n",
            "Ep 5 (Step 003960): Train loss 4.035, Val loss 4.687\n",
            "Ep 5 (Step 003965): Train loss 3.905, Val loss 4.668\n",
            "Ep 5 (Step 003970): Train loss 4.025, Val loss 4.687\n",
            "Ep 5 (Step 003975): Train loss 3.905, Val loss 4.694\n",
            "Ep 5 (Step 003980): Train loss 4.350, Val loss 4.670\n",
            "Ep 5 (Step 003985): Train loss 3.654, Val loss 4.718\n",
            "Ep 5 (Step 003990): Train loss 4.075, Val loss 4.668\n",
            "Ep 5 (Step 003995): Train loss 4.901, Val loss 4.673\n",
            "Ep 5 (Step 004000): Train loss 4.092, Val loss 4.642\n",
            "Ep 5 (Step 004005): Train loss 4.005, Val loss 4.708\n",
            "Ep 5 (Step 004010): Train loss 3.899, Val loss 4.684\n",
            "Ep 5 (Step 004015): Train loss 3.829, Val loss 4.677\n",
            "Ep 5 (Step 004020): Train loss 4.187, Val loss 4.676\n",
            "Ep 5 (Step 004025): Train loss 4.568, Val loss 4.667\n",
            "Ep 5 (Step 004030): Train loss 4.548, Val loss 4.704\n",
            "Ep 5 (Step 004035): Train loss 4.177, Val loss 4.653\n",
            "Ep 5 (Step 004040): Train loss 4.320, Val loss 4.665\n",
            "Ep 5 (Step 004045): Train loss 4.074, Val loss 4.688\n",
            "Ep 5 (Step 004050): Train loss 4.218, Val loss 4.688\n",
            "Ep 5 (Step 004055): Train loss 3.905, Val loss 4.665\n",
            "Ep 5 (Step 004060): Train loss 4.324, Val loss 4.663\n",
            "Ep 5 (Step 004065): Train loss 3.822, Val loss 4.706\n",
            "Ep 5 (Step 004070): Train loss 4.168, Val loss 4.672\n",
            "Ep 5 (Step 004075): Train loss 3.898, Val loss 4.642\n",
            "Ep 5 (Step 004080): Train loss 3.765, Val loss 4.644\n",
            "Ep 5 (Step 004085): Train loss 4.037, Val loss 4.651\n",
            "Ep 5 (Step 004090): Train loss 3.778, Val loss 4.680\n",
            "Ep 5 (Step 004095): Train loss 3.806, Val loss 4.634\n",
            "Ep 5 (Step 004100): Train loss 4.197, Val loss 4.606\n",
            "Ep 5 (Step 004105): Train loss 3.565, Val loss 4.639\n",
            "Ep 5 (Step 004110): Train loss 4.342, Val loss 4.655\n",
            "Ep 5 (Step 004115): Train loss 3.790, Val loss 4.662\n",
            "Ep 5 (Step 004120): Train loss 3.905, Val loss 4.651\n",
            "Ep 5 (Step 004125): Train loss 3.993, Val loss 4.625\n",
            "Ep 5 (Step 004130): Train loss 3.447, Val loss 4.615\n",
            "Ep 5 (Step 004135): Train loss 4.191, Val loss 4.623\n",
            "Ep 5 (Step 004140): Train loss 4.176, Val loss 4.652\n",
            "Ep 5 (Step 004145): Train loss 3.988, Val loss 4.646\n",
            "Ep 5 (Step 004150): Train loss 4.009, Val loss 4.631\n",
            "Ep 5 (Step 004155): Train loss 3.649, Val loss 4.603\n",
            "Ep 5 (Step 004160): Train loss 4.476, Val loss 4.645\n",
            "Ep 5 (Step 004165): Train loss 3.839, Val loss 4.615\n",
            "Ep 5 (Step 004170): Train loss 4.127, Val loss 4.643\n",
            "Ep 5 (Step 004175): Train loss 4.145, Val loss 4.643\n",
            "Ep 5 (Step 004180): Train loss 4.643, Val loss 4.621\n",
            "Ep 5 (Step 004185): Train loss 3.690, Val loss 4.630\n",
            "Ep 5 (Step 004190): Train loss 4.391, Val loss 4.640\n",
            "Ep 5 (Step 004195): Train loss 3.988, Val loss 4.655\n",
            "Ep 5 (Step 004200): Train loss 3.742, Val loss 4.694\n",
            "Ep 5 (Step 004205): Train loss 3.807, Val loss 4.664\n",
            "Ep 5 (Step 004210): Train loss 3.618, Val loss 4.696\n",
            "Ep 5 (Step 004215): Train loss 3.877, Val loss 4.636\n",
            "Ep 5 (Step 004220): Train loss 3.913, Val loss 4.569\n",
            "Ep 5 (Step 004225): Train loss 4.291, Val loss 4.628\n",
            "Ep 5 (Step 004230): Train loss 4.201, Val loss 4.611\n",
            "Ep 5 (Step 004235): Train loss 4.230, Val loss 4.607\n",
            "Ep 5 (Step 004240): Train loss 4.111, Val loss 4.625\n",
            "Ep 5 (Step 004245): Train loss 3.955, Val loss 4.656\n",
            "Ep 5 (Step 004250): Train loss 4.176, Val loss 4.629\n",
            "Ep 5 (Step 004255): Train loss 4.224, Val loss 4.627\n",
            "Ep 5 (Step 004260): Train loss 4.119, Val loss 4.611\n",
            "Ep 5 (Step 004265): Train loss 4.174, Val loss 4.660\n",
            "Ep 5 (Step 004270): Train loss 4.111, Val loss 4.701\n",
            "Ep 5 (Step 004275): Train loss 4.336, Val loss 4.643\n",
            "Ep 5 (Step 004280): Train loss 3.913, Val loss 4.645\n",
            "Ep 5 (Step 004285): Train loss 3.700, Val loss 4.684\n",
            "Ep 5 (Step 004290): Train loss 3.902, Val loss 4.682\n",
            "Ep 5 (Step 004295): Train loss 3.862, Val loss 4.677\n",
            "Ep 5 (Step 004300): Train loss 4.065, Val loss 4.674\n",
            "Ep 5 (Step 004305): Train loss 3.902, Val loss 4.636\n",
            "Ep 5 (Step 004310): Train loss 4.178, Val loss 4.656\n",
            "Ep 5 (Step 004315): Train loss 3.445, Val loss 4.627\n",
            "Ep 5 (Step 004320): Train loss 3.360, Val loss 4.655\n",
            "Ep 5 (Step 004325): Train loss 4.138, Val loss 4.656\n",
            "Ep 5 (Step 004330): Train loss 3.974, Val loss 4.637\n",
            "Ep 5 (Step 004335): Train loss 3.994, Val loss 4.624\n",
            "Ep 5 (Step 004340): Train loss 3.944, Val loss 4.652\n",
            "Ep 5 (Step 004345): Train loss 4.155, Val loss 4.678\n",
            "Ep 5 (Step 004350): Train loss 4.005, Val loss 4.680\n",
            "Ep 5 (Step 004355): Train loss 3.993, Val loss 4.656\n",
            "Ep 5 (Step 004360): Train loss 4.080, Val loss 4.647\n",
            "Ep 5 (Step 004365): Train loss 3.794, Val loss 4.703\n",
            "Ep 5 (Step 004370): Train loss 4.640, Val loss 4.676\n",
            "Ep 5 (Step 004375): Train loss 3.772, Val loss 4.661\n",
            "Ep 5 (Step 004380): Train loss 4.318, Val loss 4.612\n",
            "Ep 5 (Step 004385): Train loss 3.791, Val loss 4.581\n",
            "Ep 5 (Step 004390): Train loss 4.182, Val loss 4.615\n",
            "Ep 5 (Step 004395): Train loss 3.844, Val loss 4.619\n",
            "Ep 5 (Step 004400): Train loss 4.611, Val loss 4.623\n",
            "Ep 5 (Step 004405): Train loss 3.729, Val loss 4.597\n",
            "Ep 5 (Step 004410): Train loss 4.051, Val loss 4.595\n",
            "Ep 5 (Step 004415): Train loss 3.832, Val loss 4.618\n",
            "Ep 5 (Step 004420): Train loss 4.261, Val loss 4.597\n",
            "Ep 5 (Step 004425): Train loss 3.868, Val loss 4.593\n",
            "Ep 5 (Step 004430): Train loss 3.896, Val loss 4.583\n",
            "Ep 5 (Step 004435): Train loss 3.825, Val loss 4.595\n",
            "Ep 5 (Step 004440): Train loss 3.894, Val loss 4.644\n",
            "Ep 5 (Step 004445): Train loss 3.658, Val loss 4.619\n",
            "Ep 5 (Step 004450): Train loss 3.930, Val loss 4.592\n",
            "Ep 5 (Step 004455): Train loss 4.034, Val loss 4.606\n",
            "Ep 5 (Step 004460): Train loss 3.753, Val loss 4.569\n",
            "Ep 5 (Step 004465): Train loss 3.741, Val loss 4.566\n",
            "Ep 5 (Step 004470): Train loss 3.805, Val loss 4.575\n",
            "Ep 5 (Step 004475): Train loss 3.966, Val loss 4.614\n",
            "Ep 5 (Step 004480): Train loss 3.999, Val loss 4.600\n",
            "Ep 5 (Step 004485): Train loss 4.234, Val loss 4.602\n",
            "Ep 5 (Step 004490): Train loss 3.942, Val loss 4.624\n",
            "Ep 5 (Step 004495): Train loss 3.984, Val loss 4.615\n",
            "Ep 5 (Step 004500): Train loss 3.725, Val loss 4.619\n",
            "Ep 5 (Step 004505): Train loss 3.983, Val loss 4.589\n",
            "Ep 5 (Step 004510): Train loss 3.593, Val loss 4.576\n",
            "Ep 5 (Step 004515): Train loss 3.921, Val loss 4.575\n",
            "Ep 5 (Step 004520): Train loss 3.579, Val loss 4.561\n",
            "Ep 5 (Step 004525): Train loss 3.967, Val loss 4.598\n",
            "Ep 5 (Step 004530): Train loss 4.062, Val loss 4.625\n",
            "Ep 5 (Step 004535): Train loss 4.433, Val loss 4.616\n",
            "He said we came here.   I dont know that I said, said he said: I  t know that It know what It know that he said that he\n",
            "Ep 6 (Step 004540): Train loss 4.178, Val loss 4.596\n",
            "Ep 6 (Step 004545): Train loss 4.044, Val loss 4.580\n",
            "Ep 6 (Step 004550): Train loss 3.629, Val loss 4.581\n",
            "Ep 6 (Step 004555): Train loss 3.598, Val loss 4.568\n",
            "Ep 6 (Step 004560): Train loss 4.010, Val loss 4.599\n",
            "Ep 6 (Step 004565): Train loss 3.568, Val loss 4.614\n",
            "Ep 6 (Step 004570): Train loss 4.230, Val loss 4.599\n",
            "Ep 6 (Step 004575): Train loss 4.048, Val loss 4.605\n",
            "Ep 6 (Step 004580): Train loss 4.285, Val loss 4.612\n",
            "Ep 6 (Step 004585): Train loss 4.262, Val loss 4.615\n",
            "Ep 6 (Step 004590): Train loss 4.120, Val loss 4.590\n",
            "Ep 6 (Step 004595): Train loss 4.338, Val loss 4.619\n",
            "Ep 6 (Step 004600): Train loss 3.864, Val loss 4.590\n",
            "Ep 6 (Step 004605): Train loss 3.802, Val loss 4.577\n",
            "Ep 6 (Step 004610): Train loss 3.696, Val loss 4.613\n",
            "Ep 6 (Step 004615): Train loss 4.166, Val loss 4.580\n",
            "Ep 6 (Step 004620): Train loss 3.950, Val loss 4.618\n",
            "Ep 6 (Step 004625): Train loss 3.509, Val loss 4.630\n",
            "Ep 6 (Step 004630): Train loss 4.129, Val loss 4.584\n",
            "Ep 6 (Step 004635): Train loss 4.595, Val loss 4.628\n",
            "Ep 6 (Step 004640): Train loss 4.318, Val loss 4.618\n",
            "Ep 6 (Step 004645): Train loss 3.706, Val loss 4.590\n",
            "Ep 6 (Step 004650): Train loss 3.765, Val loss 4.590\n",
            "Ep 6 (Step 004655): Train loss 3.653, Val loss 4.576\n",
            "Ep 6 (Step 004660): Train loss 3.879, Val loss 4.566\n",
            "Ep 6 (Step 004665): Train loss 3.676, Val loss 4.609\n",
            "Ep 6 (Step 004670): Train loss 3.777, Val loss 4.625\n",
            "Ep 6 (Step 004675): Train loss 4.165, Val loss 4.615\n",
            "Ep 6 (Step 004680): Train loss 3.829, Val loss 4.597\n",
            "Ep 6 (Step 004685): Train loss 4.230, Val loss 4.623\n",
            "Ep 6 (Step 004690): Train loss 4.035, Val loss 4.588\n",
            "Ep 6 (Step 004695): Train loss 3.697, Val loss 4.597\n",
            "Ep 6 (Step 004700): Train loss 3.914, Val loss 4.618\n",
            "Ep 6 (Step 004705): Train loss 3.744, Val loss 4.615\n",
            "Ep 6 (Step 004710): Train loss 4.136, Val loss 4.615\n",
            "Ep 6 (Step 004715): Train loss 3.921, Val loss 4.595\n",
            "Ep 6 (Step 004720): Train loss 3.762, Val loss 4.572\n",
            "Ep 6 (Step 004725): Train loss 3.909, Val loss 4.574\n",
            "Ep 6 (Step 004730): Train loss 4.127, Val loss 4.604\n",
            "Ep 6 (Step 004735): Train loss 4.626, Val loss 4.579\n",
            "Ep 6 (Step 004740): Train loss 3.735, Val loss 4.588\n",
            "Ep 6 (Step 004745): Train loss 4.375, Val loss 4.607\n",
            "Ep 6 (Step 004750): Train loss 4.251, Val loss 4.580\n",
            "Ep 6 (Step 004755): Train loss 3.871, Val loss 4.640\n",
            "Ep 6 (Step 004760): Train loss 4.053, Val loss 4.568\n",
            "Ep 6 (Step 004765): Train loss 3.667, Val loss 4.578\n",
            "Ep 6 (Step 004770): Train loss 4.299, Val loss 4.621\n",
            "Ep 6 (Step 004775): Train loss 4.140, Val loss 4.603\n",
            "Ep 6 (Step 004780): Train loss 3.594, Val loss 4.601\n",
            "Ep 6 (Step 004785): Train loss 4.527, Val loss 4.609\n",
            "Ep 6 (Step 004790): Train loss 4.184, Val loss 4.596\n",
            "Ep 6 (Step 004795): Train loss 3.805, Val loss 4.620\n",
            "Ep 6 (Step 004800): Train loss 3.968, Val loss 4.635\n",
            "Ep 6 (Step 004805): Train loss 3.538, Val loss 4.684\n",
            "Ep 6 (Step 004810): Train loss 4.156, Val loss 4.647\n",
            "Ep 6 (Step 004815): Train loss 3.755, Val loss 4.645\n",
            "Ep 6 (Step 004820): Train loss 3.737, Val loss 4.613\n",
            "Ep 6 (Step 004825): Train loss 3.813, Val loss 4.596\n",
            "Ep 6 (Step 004830): Train loss 3.689, Val loss 4.609\n",
            "Ep 6 (Step 004835): Train loss 3.738, Val loss 4.602\n",
            "Ep 6 (Step 004840): Train loss 3.983, Val loss 4.592\n",
            "Ep 6 (Step 004845): Train loss 3.913, Val loss 4.574\n",
            "Ep 6 (Step 004850): Train loss 4.215, Val loss 4.596\n",
            "Ep 6 (Step 004855): Train loss 3.965, Val loss 4.593\n",
            "Ep 6 (Step 004860): Train loss 3.809, Val loss 4.623\n",
            "Ep 6 (Step 004865): Train loss 3.728, Val loss 4.608\n",
            "Ep 6 (Step 004870): Train loss 4.111, Val loss 4.594\n",
            "Ep 6 (Step 004875): Train loss 3.069, Val loss 4.582\n",
            "Ep 6 (Step 004880): Train loss 3.394, Val loss 4.589\n",
            "Ep 6 (Step 004885): Train loss 4.013, Val loss 4.602\n",
            "Ep 6 (Step 004890): Train loss 3.886, Val loss 4.600\n",
            "Ep 6 (Step 004895): Train loss 3.886, Val loss 4.600\n",
            "Ep 6 (Step 004900): Train loss 4.319, Val loss 4.578\n",
            "Ep 6 (Step 004905): Train loss 4.066, Val loss 4.595\n",
            "Ep 6 (Step 004910): Train loss 3.462, Val loss 4.557\n",
            "Ep 6 (Step 004915): Train loss 3.643, Val loss 4.597\n",
            "Ep 6 (Step 004920): Train loss 3.434, Val loss 4.607\n",
            "Ep 6 (Step 004925): Train loss 3.280, Val loss 4.623\n",
            "Ep 6 (Step 004930): Train loss 3.751, Val loss 4.629\n",
            "Ep 6 (Step 004935): Train loss 4.035, Val loss 4.644\n",
            "Ep 6 (Step 004940): Train loss 4.080, Val loss 4.621\n",
            "Ep 6 (Step 004945): Train loss 4.066, Val loss 4.607\n",
            "Ep 6 (Step 004950): Train loss 4.169, Val loss 4.566\n",
            "Ep 6 (Step 004955): Train loss 3.966, Val loss 4.561\n",
            "Ep 6 (Step 004960): Train loss 4.061, Val loss 4.565\n",
            "Ep 6 (Step 004965): Train loss 3.686, Val loss 4.597\n",
            "Ep 6 (Step 004970): Train loss 3.867, Val loss 4.600\n",
            "Ep 6 (Step 004975): Train loss 3.964, Val loss 4.572\n",
            "Ep 6 (Step 004980): Train loss 3.955, Val loss 4.557\n",
            "Ep 6 (Step 004985): Train loss 3.897, Val loss 4.553\n",
            "Ep 6 (Step 004990): Train loss 3.570, Val loss 4.610\n",
            "Ep 6 (Step 004995): Train loss 3.902, Val loss 4.591\n",
            "Ep 6 (Step 005000): Train loss 3.713, Val loss 4.581\n",
            "Ep 6 (Step 005005): Train loss 3.708, Val loss 4.565\n",
            "Ep 6 (Step 005010): Train loss 3.838, Val loss 4.603\n",
            "Ep 6 (Step 005015): Train loss 3.460, Val loss 4.592\n",
            "Ep 6 (Step 005020): Train loss 3.616, Val loss 4.563\n",
            "Ep 6 (Step 005025): Train loss 3.451, Val loss 4.605\n",
            "Ep 6 (Step 005030): Train loss 3.947, Val loss 4.617\n",
            "Ep 6 (Step 005035): Train loss 3.603, Val loss 4.606\n",
            "Ep 6 (Step 005040): Train loss 3.818, Val loss 4.549\n",
            "Ep 6 (Step 005045): Train loss 3.268, Val loss 4.536\n",
            "Ep 6 (Step 005050): Train loss 3.382, Val loss 4.535\n",
            "Ep 6 (Step 005055): Train loss 3.884, Val loss 4.534\n",
            "Ep 6 (Step 005060): Train loss 4.054, Val loss 4.550\n",
            "Ep 6 (Step 005065): Train loss 3.859, Val loss 4.536\n",
            "Ep 6 (Step 005070): Train loss 3.921, Val loss 4.554\n",
            "Ep 6 (Step 005075): Train loss 4.141, Val loss 4.542\n",
            "Ep 6 (Step 005080): Train loss 3.917, Val loss 4.551\n",
            "Ep 6 (Step 005085): Train loss 3.614, Val loss 4.544\n",
            "Ep 6 (Step 005090): Train loss 4.193, Val loss 4.527\n",
            "Ep 6 (Step 005095): Train loss 3.812, Val loss 4.526\n",
            "Ep 6 (Step 005100): Train loss 4.299, Val loss 4.559\n",
            "Ep 6 (Step 005105): Train loss 3.137, Val loss 4.573\n",
            "Ep 6 (Step 005110): Train loss 4.083, Val loss 4.559\n",
            "Ep 6 (Step 005115): Train loss 3.991, Val loss 4.558\n",
            "Ep 6 (Step 005120): Train loss 3.239, Val loss 4.572\n",
            "Ep 6 (Step 005125): Train loss 4.030, Val loss 4.584\n",
            "Ep 6 (Step 005130): Train loss 3.910, Val loss 4.569\n",
            "Ep 6 (Step 005135): Train loss 3.495, Val loss 4.572\n",
            "Ep 6 (Step 005140): Train loss 3.719, Val loss 4.551\n",
            "Ep 6 (Step 005145): Train loss 3.596, Val loss 4.598\n",
            "Ep 6 (Step 005150): Train loss 4.131, Val loss 4.601\n",
            "Ep 6 (Step 005155): Train loss 3.911, Val loss 4.585\n",
            "Ep 6 (Step 005160): Train loss 3.599, Val loss 4.558\n",
            "Ep 6 (Step 005165): Train loss 3.787, Val loss 4.541\n",
            "Ep 6 (Step 005170): Train loss 3.368, Val loss 4.533\n",
            "Ep 6 (Step 005175): Train loss 3.790, Val loss 4.558\n",
            "Ep 6 (Step 005180): Train loss 3.807, Val loss 4.515\n",
            "Ep 6 (Step 005185): Train loss 3.319, Val loss 4.518\n",
            "Ep 6 (Step 005190): Train loss 3.834, Val loss 4.551\n",
            "Ep 6 (Step 005195): Train loss 3.694, Val loss 4.544\n",
            "Ep 6 (Step 005200): Train loss 3.568, Val loss 4.563\n",
            "Ep 6 (Step 005205): Train loss 3.866, Val loss 4.527\n",
            "Ep 6 (Step 005210): Train loss 4.075, Val loss 4.549\n",
            "Ep 6 (Step 005215): Train loss 3.442, Val loss 4.535\n",
            "Ep 6 (Step 005220): Train loss 4.067, Val loss 4.553\n",
            "Ep 6 (Step 005225): Train loss 3.784, Val loss 4.537\n",
            "Ep 6 (Step 005230): Train loss 3.991, Val loss 4.509\n",
            "Ep 6 (Step 005235): Train loss 3.406, Val loss 4.498\n",
            "Ep 6 (Step 005240): Train loss 4.054, Val loss 4.513\n",
            "Ep 6 (Step 005245): Train loss 3.524, Val loss 4.542\n",
            "Ep 6 (Step 005250): Train loss 3.425, Val loss 4.541\n",
            "Ep 6 (Step 005255): Train loss 3.707, Val loss 4.523\n",
            "Ep 6 (Step 005260): Train loss 3.649, Val loss 4.536\n",
            "Ep 6 (Step 005265): Train loss 3.540, Val loss 4.518\n",
            "Ep 6 (Step 005270): Train loss 3.558, Val loss 4.508\n",
            "Ep 6 (Step 005275): Train loss 3.528, Val loss 4.527\n",
            "Ep 6 (Step 005280): Train loss 4.163, Val loss 4.544\n",
            "Ep 6 (Step 005285): Train loss 3.995, Val loss 4.559\n",
            "Ep 6 (Step 005290): Train loss 3.358, Val loss 4.522\n",
            "Ep 6 (Step 005295): Train loss 3.747, Val loss 4.574\n",
            "Ep 6 (Step 005300): Train loss 3.876, Val loss 4.535\n",
            "Ep 6 (Step 005305): Train loss 3.920, Val loss 4.524\n",
            "Ep 6 (Step 005310): Train loss 3.442, Val loss 4.561\n",
            "Ep 6 (Step 005315): Train loss 3.524, Val loss 4.551\n",
            "Ep 6 (Step 005320): Train loss 3.950, Val loss 4.577\n",
            "Ep 6 (Step 005325): Train loss 3.741, Val loss 4.530\n",
            "Ep 6 (Step 005330): Train loss 3.964, Val loss 4.518\n",
            "Ep 6 (Step 005335): Train loss 4.322, Val loss 4.536\n",
            "Ep 6 (Step 005340): Train loss 3.844, Val loss 4.546\n",
            "Ep 6 (Step 005345): Train loss 3.661, Val loss 4.511\n",
            "Ep 6 (Step 005350): Train loss 3.565, Val loss 4.546\n",
            "Ep 6 (Step 005355): Train loss 3.649, Val loss 4.493\n",
            "Ep 6 (Step 005360): Train loss 3.386, Val loss 4.505\n",
            "Ep 6 (Step 005365): Train loss 3.892, Val loss 4.526\n",
            "Ep 6 (Step 005370): Train loss 3.707, Val loss 4.499\n",
            "Ep 6 (Step 005375): Train loss 3.834, Val loss 4.516\n",
            "Ep 6 (Step 005380): Train loss 3.698, Val loss 4.519\n",
            "Ep 6 (Step 005385): Train loss 3.720, Val loss 4.502\n",
            "Ep 6 (Step 005390): Train loss 3.183, Val loss 4.537\n",
            "Ep 6 (Step 005395): Train loss 3.762, Val loss 4.517\n",
            "Ep 6 (Step 005400): Train loss 3.847, Val loss 4.503\n",
            "Ep 6 (Step 005405): Train loss 3.241, Val loss 4.520\n",
            "Ep 6 (Step 005410): Train loss 3.734, Val loss 4.522\n",
            "Ep 6 (Step 005415): Train loss 3.445, Val loss 4.536\n",
            "Ep 6 (Step 005420): Train loss 3.529, Val loss 4.556\n",
            "Ep 6 (Step 005425): Train loss 3.443, Val loss 4.498\n",
            "Ep 6 (Step 005430): Train loss 3.710, Val loss 4.521\n",
            "Ep 6 (Step 005435): Train loss 3.666, Val loss 4.559\n",
            "Ep 6 (Step 005440): Train loss 3.840, Val loss 4.532\n",
            "Ep 6 (Step 005445): Train loss 3.981, Val loss 4.545\n",
            "He said we came here.  I dont know what It know,t know.  It know what It know what It know that I\n",
            "Ep 7 (Step 005450): Train loss 3.577, Val loss 4.536\n",
            "Ep 7 (Step 005455): Train loss 4.015, Val loss 4.523\n",
            "Ep 7 (Step 005460): Train loss 3.461, Val loss 4.546\n",
            "Ep 7 (Step 005465): Train loss 4.129, Val loss 4.512\n",
            "Ep 7 (Step 005470): Train loss 3.953, Val loss 4.519\n",
            "Ep 7 (Step 005475): Train loss 3.597, Val loss 4.499\n",
            "Ep 7 (Step 005480): Train loss 3.388, Val loss 4.503\n",
            "Ep 7 (Step 005485): Train loss 3.820, Val loss 4.547\n",
            "Ep 7 (Step 005490): Train loss 3.754, Val loss 4.544\n",
            "Ep 7 (Step 005495): Train loss 3.700, Val loss 4.558\n",
            "Ep 7 (Step 005500): Train loss 3.302, Val loss 4.543\n",
            "Ep 7 (Step 005505): Train loss 3.517, Val loss 4.538\n",
            "Ep 7 (Step 005510): Train loss 3.903, Val loss 4.561\n",
            "Ep 7 (Step 005515): Train loss 3.824, Val loss 4.586\n",
            "Ep 7 (Step 005520): Train loss 3.331, Val loss 4.510\n",
            "Ep 7 (Step 005525): Train loss 3.910, Val loss 4.512\n",
            "Ep 7 (Step 005530): Train loss 3.341, Val loss 4.556\n",
            "Ep 7 (Step 005535): Train loss 3.549, Val loss 4.561\n",
            "Ep 7 (Step 005540): Train loss 3.994, Val loss 4.564\n",
            "Ep 7 (Step 005545): Train loss 3.979, Val loss 4.567\n",
            "Ep 7 (Step 005550): Train loss 3.916, Val loss 4.568\n",
            "Ep 7 (Step 005555): Train loss 3.771, Val loss 4.578\n",
            "Ep 7 (Step 005560): Train loss 3.664, Val loss 4.550\n",
            "Ep 7 (Step 005565): Train loss 3.447, Val loss 4.521\n",
            "Ep 7 (Step 005570): Train loss 3.618, Val loss 4.545\n",
            "Ep 7 (Step 005575): Train loss 3.447, Val loss 4.519\n",
            "Ep 7 (Step 005580): Train loss 3.481, Val loss 4.554\n",
            "Ep 7 (Step 005585): Train loss 3.891, Val loss 4.514\n",
            "Ep 7 (Step 005590): Train loss 3.303, Val loss 4.498\n",
            "Ep 7 (Step 005595): Train loss 3.820, Val loss 4.520\n",
            "Ep 7 (Step 005600): Train loss 3.578, Val loss 4.530\n",
            "Ep 7 (Step 005605): Train loss 3.457, Val loss 4.556\n",
            "Ep 7 (Step 005610): Train loss 3.475, Val loss 4.592\n",
            "Ep 7 (Step 005615): Train loss 3.804, Val loss 4.555\n",
            "Ep 7 (Step 005620): Train loss 3.783, Val loss 4.555\n",
            "Ep 7 (Step 005625): Train loss 3.611, Val loss 4.582\n",
            "Ep 7 (Step 005630): Train loss 4.144, Val loss 4.544\n",
            "Ep 7 (Step 005635): Train loss 3.807, Val loss 4.572\n",
            "Ep 7 (Step 005640): Train loss 3.745, Val loss 4.541\n",
            "Ep 7 (Step 005645): Train loss 3.509, Val loss 4.558\n",
            "Ep 7 (Step 005650): Train loss 3.407, Val loss 4.543\n",
            "Ep 7 (Step 005655): Train loss 3.758, Val loss 4.548\n",
            "Ep 7 (Step 005660): Train loss 3.244, Val loss 4.565\n",
            "Ep 7 (Step 005665): Train loss 3.888, Val loss 4.576\n",
            "Ep 7 (Step 005670): Train loss 4.135, Val loss 4.585\n",
            "Ep 7 (Step 005675): Train loss 3.644, Val loss 4.550\n",
            "Ep 7 (Step 005680): Train loss 3.896, Val loss 4.566\n",
            "Ep 7 (Step 005685): Train loss 3.956, Val loss 4.545\n",
            "Ep 7 (Step 005690): Train loss 3.657, Val loss 4.534\n",
            "Ep 7 (Step 005695): Train loss 3.585, Val loss 4.548\n",
            "Ep 7 (Step 005700): Train loss 4.201, Val loss 4.512\n",
            "Ep 7 (Step 005705): Train loss 3.886, Val loss 4.537\n",
            "Ep 7 (Step 005710): Train loss 3.611, Val loss 4.562\n",
            "Ep 7 (Step 005715): Train loss 3.717, Val loss 4.517\n",
            "Ep 7 (Step 005720): Train loss 3.932, Val loss 4.518\n",
            "Ep 7 (Step 005725): Train loss 3.454, Val loss 4.549\n",
            "Ep 7 (Step 005730): Train loss 4.169, Val loss 4.535\n",
            "Ep 7 (Step 005735): Train loss 3.769, Val loss 4.563\n",
            "Ep 7 (Step 005740): Train loss 3.820, Val loss 4.560\n",
            "Ep 7 (Step 005745): Train loss 4.014, Val loss 4.589\n",
            "Ep 7 (Step 005750): Train loss 3.546, Val loss 4.636\n",
            "Ep 7 (Step 005755): Train loss 3.687, Val loss 4.607\n",
            "Ep 7 (Step 005760): Train loss 3.311, Val loss 4.590\n",
            "Ep 7 (Step 005765): Train loss 3.647, Val loss 4.575\n",
            "Ep 7 (Step 005770): Train loss 3.383, Val loss 4.580\n",
            "Ep 7 (Step 005775): Train loss 3.557, Val loss 4.577\n",
            "Ep 7 (Step 005780): Train loss 3.328, Val loss 4.573\n",
            "Ep 7 (Step 005785): Train loss 3.617, Val loss 4.583\n",
            "Ep 7 (Step 005790): Train loss 3.809, Val loss 4.595\n",
            "Ep 7 (Step 005795): Train loss 3.964, Val loss 4.586\n",
            "Ep 7 (Step 005800): Train loss 3.555, Val loss 4.580\n",
            "Ep 7 (Step 005805): Train loss 3.291, Val loss 4.549\n",
            "Ep 7 (Step 005810): Train loss 3.625, Val loss 4.594\n",
            "Ep 7 (Step 005815): Train loss 3.663, Val loss 4.591\n",
            "Ep 7 (Step 005820): Train loss 3.149, Val loss 4.565\n",
            "Ep 7 (Step 005825): Train loss 4.032, Val loss 4.569\n",
            "Ep 7 (Step 005830): Train loss 3.243, Val loss 4.551\n",
            "Ep 7 (Step 005835): Train loss 3.860, Val loss 4.596\n",
            "Ep 7 (Step 005840): Train loss 3.942, Val loss 4.600\n",
            "Ep 7 (Step 005845): Train loss 3.550, Val loss 4.555\n",
            "Ep 7 (Step 005850): Train loss 3.632, Val loss 4.558\n",
            "Ep 7 (Step 005855): Train loss 3.757, Val loss 4.533\n",
            "Ep 7 (Step 005860): Train loss 3.191, Val loss 4.545\n",
            "Ep 7 (Step 005865): Train loss 3.204, Val loss 4.564\n",
            "Ep 7 (Step 005870): Train loss 3.502, Val loss 4.557\n",
            "Ep 7 (Step 005875): Train loss 3.583, Val loss 4.589\n",
            "Ep 7 (Step 005880): Train loss 3.796, Val loss 4.595\n",
            "Ep 7 (Step 005885): Train loss 3.282, Val loss 4.607\n",
            "Ep 7 (Step 005890): Train loss 3.727, Val loss 4.613\n",
            "Ep 7 (Step 005895): Train loss 3.419, Val loss 4.599\n",
            "Ep 7 (Step 005900): Train loss 3.679, Val loss 4.562\n",
            "Ep 7 (Step 005905): Train loss 3.458, Val loss 4.557\n",
            "Ep 7 (Step 005910): Train loss 3.641, Val loss 4.554\n",
            "Ep 7 (Step 005915): Train loss 3.083, Val loss 4.541\n",
            "Ep 7 (Step 005920): Train loss 3.705, Val loss 4.541\n",
            "Ep 7 (Step 005925): Train loss 3.875, Val loss 4.551\n",
            "Ep 7 (Step 005930): Train loss 3.167, Val loss 4.547\n",
            "Ep 7 (Step 005935): Train loss 3.529, Val loss 4.537\n",
            "Ep 7 (Step 005940): Train loss 3.607, Val loss 4.538\n",
            "Ep 7 (Step 005945): Train loss 3.574, Val loss 4.538\n",
            "Ep 7 (Step 005950): Train loss 3.644, Val loss 4.554\n",
            "Ep 7 (Step 005955): Train loss 3.577, Val loss 4.540\n",
            "Ep 7 (Step 005960): Train loss 3.342, Val loss 4.553\n",
            "Ep 7 (Step 005965): Train loss 4.096, Val loss 4.568\n",
            "Ep 7 (Step 005970): Train loss 3.169, Val loss 4.553\n",
            "Ep 7 (Step 005975): Train loss 3.339, Val loss 4.509\n",
            "Ep 7 (Step 005980): Train loss 3.509, Val loss 4.507\n",
            "Ep 7 (Step 005985): Train loss 3.298, Val loss 4.527\n",
            "Ep 7 (Step 005990): Train loss 3.444, Val loss 4.563\n",
            "Ep 7 (Step 005995): Train loss 3.134, Val loss 4.546\n",
            "Ep 7 (Step 006000): Train loss 3.484, Val loss 4.535\n",
            "Ep 7 (Step 006005): Train loss 3.560, Val loss 4.571\n",
            "Ep 7 (Step 006010): Train loss 3.085, Val loss 4.565\n",
            "Ep 7 (Step 006015): Train loss 3.573, Val loss 4.492\n",
            "Ep 7 (Step 006020): Train loss 3.696, Val loss 4.480\n",
            "Ep 7 (Step 006025): Train loss 3.477, Val loss 4.481\n",
            "Ep 7 (Step 006030): Train loss 3.588, Val loss 4.491\n",
            "Ep 7 (Step 006035): Train loss 3.724, Val loss 4.495\n",
            "Ep 7 (Step 006040): Train loss 3.191, Val loss 4.491\n",
            "Ep 7 (Step 006045): Train loss 3.236, Val loss 4.485\n",
            "Ep 7 (Step 006050): Train loss 3.177, Val loss 4.515\n",
            "Ep 7 (Step 006055): Train loss 3.348, Val loss 4.516\n",
            "Ep 7 (Step 006060): Train loss 3.803, Val loss 4.530\n",
            "Ep 7 (Step 006065): Train loss 3.730, Val loss 4.520\n",
            "Ep 7 (Step 006070): Train loss 3.195, Val loss 4.544\n",
            "Ep 7 (Step 006075): Train loss 3.305, Val loss 4.551\n",
            "Ep 7 (Step 006080): Train loss 3.429, Val loss 4.537\n",
            "Ep 7 (Step 006085): Train loss 3.440, Val loss 4.553\n",
            "Ep 7 (Step 006090): Train loss 3.725, Val loss 4.516\n",
            "Ep 7 (Step 006095): Train loss 3.390, Val loss 4.538\n",
            "Ep 7 (Step 006100): Train loss 3.160, Val loss 4.547\n",
            "Ep 7 (Step 006105): Train loss 3.475, Val loss 4.587\n",
            "Ep 7 (Step 006110): Train loss 3.536, Val loss 4.536\n",
            "Ep 7 (Step 006115): Train loss 3.855, Val loss 4.508\n",
            "Ep 7 (Step 006120): Train loss 3.216, Val loss 4.507\n",
            "Ep 7 (Step 006125): Train loss 3.957, Val loss 4.491\n",
            "Ep 7 (Step 006130): Train loss 3.363, Val loss 4.501\n",
            "Ep 7 (Step 006135): Train loss 3.246, Val loss 4.524\n",
            "Ep 7 (Step 006140): Train loss 3.743, Val loss 4.500\n",
            "Ep 7 (Step 006145): Train loss 3.153, Val loss 4.535\n",
            "Ep 7 (Step 006150): Train loss 3.145, Val loss 4.551\n",
            "Ep 7 (Step 006155): Train loss 3.061, Val loss 4.527\n",
            "Ep 7 (Step 006160): Train loss 3.802, Val loss 4.505\n",
            "Ep 7 (Step 006165): Train loss 3.135, Val loss 4.532\n",
            "Ep 7 (Step 006170): Train loss 3.788, Val loss 4.544\n",
            "Ep 7 (Step 006175): Train loss 3.563, Val loss 4.553\n",
            "Ep 7 (Step 006180): Train loss 3.517, Val loss 4.533\n",
            "Ep 7 (Step 006185): Train loss 3.656, Val loss 4.543\n",
            "Ep 7 (Step 006190): Train loss 3.610, Val loss 4.535\n",
            "Ep 7 (Step 006195): Train loss 3.404, Val loss 4.540\n",
            "Ep 7 (Step 006200): Train loss 3.775, Val loss 4.574\n",
            "Ep 7 (Step 006205): Train loss 3.425, Val loss 4.552\n",
            "Ep 7 (Step 006210): Train loss 3.199, Val loss 4.579\n",
            "Ep 7 (Step 006215): Train loss 3.366, Val loss 4.557\n",
            "Ep 7 (Step 006220): Train loss 3.661, Val loss 4.532\n",
            "Ep 7 (Step 006225): Train loss 3.256, Val loss 4.545\n",
            "Ep 7 (Step 006230): Train loss 3.063, Val loss 4.565\n",
            "Ep 7 (Step 006235): Train loss 3.612, Val loss 4.575\n",
            "Ep 7 (Step 006240): Train loss 3.674, Val loss 4.567\n",
            "Ep 7 (Step 006245): Train loss 3.780, Val loss 4.567\n",
            "Ep 7 (Step 006250): Train loss 3.223, Val loss 4.512\n",
            "Ep 7 (Step 006255): Train loss 3.708, Val loss 4.524\n",
            "Ep 7 (Step 006260): Train loss 3.333, Val loss 4.554\n",
            "Ep 7 (Step 006265): Train loss 3.204, Val loss 4.561\n",
            "Ep 7 (Step 006270): Train loss 3.078, Val loss 4.537\n",
            "Ep 7 (Step 006275): Train loss 3.234, Val loss 4.556\n",
            "Ep 7 (Step 006280): Train loss 3.052, Val loss 4.529\n",
            "Ep 7 (Step 006285): Train loss 3.312, Val loss 4.494\n",
            "Ep 7 (Step 006290): Train loss 3.452, Val loss 4.509\n",
            "Ep 7 (Step 006295): Train loss 3.587, Val loss 4.531\n",
            "Ep 7 (Step 006300): Train loss 3.420, Val loss 4.529\n",
            "Ep 7 (Step 006305): Train loss 3.697, Val loss 4.538\n",
            "Ep 7 (Step 006310): Train loss 2.984, Val loss 4.547\n",
            "Ep 7 (Step 006315): Train loss 3.389, Val loss 4.522\n",
            "Ep 7 (Step 006320): Train loss 3.184, Val loss 4.548\n",
            "Ep 7 (Step 006325): Train loss 3.405, Val loss 4.549\n",
            "Ep 7 (Step 006330): Train loss 3.384, Val loss 4.521\n",
            "Ep 7 (Step 006335): Train loss 3.636, Val loss 4.536\n",
            "Ep 7 (Step 006340): Train loss 3.619, Val loss 4.541\n",
            "Ep 7 (Step 006345): Train loss 3.659, Val loss 4.545\n",
            "Ep 7 (Step 006350): Train loss 3.219, Val loss 4.566\n",
            "Ep 7 (Step 006355): Train loss 3.200, Val loss 4.535\n",
            "He said we came here.  I dont know that I dont know,t know,t know that It know that It know that it was at\n",
            "Ep 8 (Step 006360): Train loss 3.403, Val loss 4.544\n",
            "Ep 8 (Step 006365): Train loss 3.243, Val loss 4.520\n",
            "Ep 8 (Step 006370): Train loss 3.536, Val loss 4.523\n",
            "Ep 8 (Step 006375): Train loss 3.197, Val loss 4.542\n",
            "Ep 8 (Step 006380): Train loss 3.435, Val loss 4.524\n",
            "Ep 8 (Step 006385): Train loss 3.670, Val loss 4.551\n",
            "Ep 8 (Step 006390): Train loss 3.504, Val loss 4.509\n",
            "Ep 8 (Step 006395): Train loss 3.703, Val loss 4.512\n",
            "Ep 8 (Step 006400): Train loss 3.563, Val loss 4.543\n",
            "Ep 8 (Step 006405): Train loss 3.669, Val loss 4.555\n",
            "Ep 8 (Step 006410): Train loss 3.279, Val loss 4.565\n",
            "Ep 8 (Step 006415): Train loss 3.146, Val loss 4.586\n",
            "Ep 8 (Step 006420): Train loss 3.375, Val loss 4.564\n",
            "Ep 8 (Step 006425): Train loss 3.329, Val loss 4.538\n",
            "Ep 8 (Step 006430): Train loss 3.715, Val loss 4.549\n",
            "Ep 8 (Step 006435): Train loss 3.631, Val loss 4.567\n",
            "Ep 8 (Step 006440): Train loss 3.429, Val loss 4.524\n",
            "Ep 8 (Step 006445): Train loss 3.695, Val loss 4.542\n",
            "Ep 8 (Step 006450): Train loss 3.444, Val loss 4.562\n",
            "Ep 8 (Step 006455): Train loss 3.393, Val loss 4.540\n",
            "Ep 8 (Step 006460): Train loss 3.267, Val loss 4.576\n",
            "Ep 8 (Step 006465): Train loss 3.063, Val loss 4.543\n",
            "Ep 8 (Step 006470): Train loss 3.244, Val loss 4.550\n",
            "Ep 8 (Step 006475): Train loss 3.462, Val loss 4.592\n",
            "Ep 8 (Step 006480): Train loss 2.935, Val loss 4.568\n",
            "Ep 8 (Step 006485): Train loss 3.361, Val loss 4.554\n",
            "Ep 8 (Step 006490): Train loss 3.526, Val loss 4.548\n",
            "Ep 8 (Step 006495): Train loss 3.187, Val loss 4.545\n",
            "Ep 8 (Step 006500): Train loss 3.928, Val loss 4.572\n",
            "Ep 8 (Step 006505): Train loss 3.074, Val loss 4.584\n",
            "Ep 8 (Step 006510): Train loss 3.013, Val loss 4.603\n",
            "Ep 8 (Step 006515): Train loss 3.264, Val loss 4.569\n",
            "Ep 8 (Step 006520): Train loss 3.206, Val loss 4.577\n",
            "Ep 8 (Step 006525): Train loss 3.572, Val loss 4.596\n",
            "Ep 8 (Step 006530): Train loss 3.554, Val loss 4.617\n",
            "Ep 8 (Step 006535): Train loss 2.934, Val loss 4.562\n",
            "Ep 8 (Step 006540): Train loss 3.725, Val loss 4.536\n",
            "Ep 8 (Step 006545): Train loss 3.802, Val loss 4.546\n",
            "Ep 8 (Step 006550): Train loss 3.077, Val loss 4.559\n",
            "Ep 8 (Step 006555): Train loss 3.390, Val loss 4.566\n",
            "Ep 8 (Step 006560): Train loss 3.009, Val loss 4.589\n",
            "Ep 8 (Step 006565): Train loss 3.341, Val loss 4.568\n",
            "Ep 8 (Step 006570): Train loss 3.488, Val loss 4.543\n",
            "Ep 8 (Step 006575): Train loss 3.349, Val loss 4.514\n",
            "Ep 8 (Step 006580): Train loss 3.504, Val loss 4.546\n",
            "Ep 8 (Step 006585): Train loss 2.969, Val loss 4.562\n",
            "Ep 8 (Step 006590): Train loss 3.159, Val loss 4.595\n",
            "Ep 8 (Step 006595): Train loss 3.329, Val loss 4.561\n",
            "Ep 8 (Step 006600): Train loss 3.381, Val loss 4.518\n",
            "Ep 8 (Step 006605): Train loss 3.337, Val loss 4.511\n",
            "Ep 8 (Step 006610): Train loss 3.380, Val loss 4.526\n",
            "Ep 8 (Step 006615): Train loss 2.939, Val loss 4.553\n",
            "Ep 8 (Step 006620): Train loss 3.352, Val loss 4.556\n",
            "Ep 8 (Step 006625): Train loss 3.663, Val loss 4.548\n",
            "Ep 8 (Step 006630): Train loss 3.853, Val loss 4.541\n",
            "Ep 8 (Step 006635): Train loss 3.290, Val loss 4.567\n",
            "Ep 8 (Step 006640): Train loss 3.133, Val loss 4.562\n",
            "Ep 8 (Step 006645): Train loss 3.269, Val loss 4.562\n",
            "Ep 8 (Step 006650): Train loss 2.989, Val loss 4.587\n",
            "Ep 8 (Step 006655): Train loss 3.275, Val loss 4.608\n",
            "Ep 8 (Step 006660): Train loss 2.906, Val loss 4.588\n",
            "Ep 8 (Step 006665): Train loss 3.719, Val loss 4.592\n",
            "Ep 8 (Step 006670): Train loss 3.293, Val loss 4.590\n",
            "Ep 8 (Step 006675): Train loss 3.393, Val loss 4.556\n",
            "Ep 8 (Step 006680): Train loss 3.260, Val loss 4.566\n",
            "Ep 8 (Step 006685): Train loss 3.306, Val loss 4.555\n",
            "Ep 8 (Step 006690): Train loss 3.385, Val loss 4.584\n",
            "Ep 8 (Step 006695): Train loss 3.321, Val loss 4.596\n",
            "Ep 8 (Step 006700): Train loss 3.372, Val loss 4.556\n",
            "Ep 8 (Step 006705): Train loss 3.256, Val loss 4.564\n",
            "Ep 8 (Step 006710): Train loss 3.101, Val loss 4.548\n",
            "Ep 8 (Step 006715): Train loss 3.514, Val loss 4.538\n",
            "Ep 8 (Step 006720): Train loss 3.377, Val loss 4.535\n",
            "Ep 8 (Step 006725): Train loss 3.209, Val loss 4.522\n",
            "Ep 8 (Step 006730): Train loss 3.476, Val loss 4.510\n",
            "Ep 8 (Step 006735): Train loss 3.360, Val loss 4.533\n",
            "Ep 8 (Step 006740): Train loss 3.144, Val loss 4.520\n",
            "Ep 8 (Step 006745): Train loss 3.212, Val loss 4.511\n",
            "Ep 8 (Step 006750): Train loss 2.824, Val loss 4.528\n",
            "Ep 8 (Step 006755): Train loss 3.595, Val loss 4.543\n",
            "Ep 8 (Step 006760): Train loss 2.996, Val loss 4.537\n",
            "Ep 8 (Step 006765): Train loss 3.207, Val loss 4.539\n",
            "Ep 8 (Step 006770): Train loss 3.331, Val loss 4.541\n",
            "Ep 8 (Step 006775): Train loss 3.371, Val loss 4.543\n",
            "Ep 8 (Step 006780): Train loss 3.312, Val loss 4.548\n",
            "Ep 8 (Step 006785): Train loss 2.845, Val loss 4.543\n",
            "Ep 8 (Step 006790): Train loss 3.131, Val loss 4.524\n",
            "Ep 8 (Step 006795): Train loss 3.294, Val loss 4.516\n",
            "Ep 8 (Step 006800): Train loss 3.363, Val loss 4.530\n",
            "Ep 8 (Step 006805): Train loss 3.788, Val loss 4.509\n",
            "Ep 8 (Step 006810): Train loss 3.640, Val loss 4.519\n",
            "Ep 8 (Step 006815): Train loss 3.441, Val loss 4.513\n",
            "Ep 8 (Step 006820): Train loss 3.051, Val loss 4.526\n",
            "Ep 8 (Step 006825): Train loss 3.532, Val loss 4.564\n",
            "Ep 8 (Step 006830): Train loss 3.181, Val loss 4.580\n",
            "Ep 8 (Step 006835): Train loss 3.417, Val loss 4.569\n",
            "Ep 8 (Step 006840): Train loss 3.610, Val loss 4.579\n",
            "Ep 8 (Step 006845): Train loss 3.235, Val loss 4.515\n",
            "Ep 8 (Step 006850): Train loss 3.033, Val loss 4.524\n",
            "Ep 8 (Step 006855): Train loss 3.118, Val loss 4.557\n",
            "Ep 8 (Step 006860): Train loss 3.184, Val loss 4.590\n",
            "Ep 8 (Step 006865): Train loss 3.273, Val loss 4.576\n",
            "Ep 8 (Step 006870): Train loss 3.187, Val loss 4.536\n",
            "Ep 8 (Step 006875): Train loss 3.539, Val loss 4.575\n",
            "Ep 8 (Step 006880): Train loss 3.239, Val loss 4.557\n",
            "Ep 8 (Step 006885): Train loss 2.931, Val loss 4.540\n",
            "Ep 8 (Step 006890): Train loss 3.000, Val loss 4.530\n",
            "Ep 8 (Step 006895): Train loss 3.338, Val loss 4.524\n",
            "Ep 8 (Step 006900): Train loss 3.285, Val loss 4.570\n",
            "Ep 8 (Step 006905): Train loss 2.989, Val loss 4.552\n",
            "Ep 8 (Step 006910): Train loss 2.975, Val loss 4.511\n",
            "Ep 8 (Step 006915): Train loss 3.334, Val loss 4.558\n",
            "Ep 8 (Step 006920): Train loss 3.229, Val loss 4.537\n",
            "Ep 8 (Step 006925): Train loss 3.250, Val loss 4.550\n",
            "Ep 8 (Step 006930): Train loss 3.267, Val loss 4.581\n",
            "Ep 8 (Step 006935): Train loss 3.465, Val loss 4.544\n",
            "Ep 8 (Step 006940): Train loss 3.245, Val loss 4.525\n",
            "Ep 8 (Step 006945): Train loss 2.963, Val loss 4.575\n",
            "Ep 8 (Step 006950): Train loss 3.113, Val loss 4.535\n",
            "Ep 8 (Step 006955): Train loss 3.687, Val loss 4.554\n",
            "Ep 8 (Step 006960): Train loss 3.288, Val loss 4.553\n",
            "Ep 8 (Step 006965): Train loss 3.325, Val loss 4.548\n",
            "Ep 8 (Step 006970): Train loss 3.403, Val loss 4.536\n",
            "Ep 8 (Step 006975): Train loss 3.273, Val loss 4.602\n",
            "Ep 8 (Step 006980): Train loss 2.964, Val loss 4.590\n",
            "Ep 8 (Step 006985): Train loss 3.277, Val loss 4.563\n",
            "Ep 8 (Step 006990): Train loss 3.328, Val loss 4.569\n",
            "Ep 8 (Step 006995): Train loss 3.385, Val loss 4.542\n",
            "Ep 8 (Step 007000): Train loss 3.043, Val loss 4.556\n",
            "Ep 8 (Step 007005): Train loss 3.335, Val loss 4.578\n",
            "Ep 8 (Step 007010): Train loss 3.440, Val loss 4.563\n",
            "Ep 8 (Step 007015): Train loss 3.493, Val loss 4.556\n",
            "Ep 8 (Step 007020): Train loss 3.495, Val loss 4.549\n",
            "Ep 8 (Step 007025): Train loss 3.469, Val loss 4.560\n",
            "Ep 8 (Step 007030): Train loss 3.503, Val loss 4.558\n",
            "Ep 8 (Step 007035): Train loss 3.394, Val loss 4.548\n",
            "Ep 8 (Step 007040): Train loss 2.976, Val loss 4.569\n",
            "Ep 8 (Step 007045): Train loss 3.278, Val loss 4.565\n",
            "Ep 8 (Step 007050): Train loss 3.269, Val loss 4.587\n",
            "Ep 8 (Step 007055): Train loss 2.916, Val loss 4.615\n",
            "Ep 8 (Step 007060): Train loss 3.256, Val loss 4.560\n",
            "Ep 8 (Step 007065): Train loss 3.446, Val loss 4.549\n",
            "Ep 8 (Step 007070): Train loss 2.961, Val loss 4.552\n",
            "Ep 8 (Step 007075): Train loss 3.324, Val loss 4.570\n",
            "Ep 8 (Step 007080): Train loss 3.413, Val loss 4.579\n",
            "Ep 8 (Step 007085): Train loss 2.628, Val loss 4.558\n",
            "Ep 8 (Step 007090): Train loss 3.544, Val loss 4.595\n",
            "Ep 8 (Step 007095): Train loss 3.146, Val loss 4.609\n",
            "Ep 8 (Step 007100): Train loss 3.136, Val loss 4.615\n",
            "Ep 8 (Step 007105): Train loss 3.213, Val loss 4.547\n",
            "Ep 8 (Step 007110): Train loss 3.412, Val loss 4.566\n",
            "Ep 8 (Step 007115): Train loss 3.266, Val loss 4.558\n",
            "Ep 8 (Step 007120): Train loss 3.006, Val loss 4.535\n",
            "Ep 8 (Step 007125): Train loss 2.936, Val loss 4.558\n",
            "Ep 8 (Step 007130): Train loss 2.967, Val loss 4.578\n",
            "Ep 8 (Step 007135): Train loss 3.080, Val loss 4.543\n",
            "Ep 8 (Step 007140): Train loss 3.022, Val loss 4.561\n",
            "Ep 8 (Step 007145): Train loss 3.211, Val loss 4.542\n",
            "Ep 8 (Step 007150): Train loss 3.118, Val loss 4.542\n",
            "Ep 8 (Step 007155): Train loss 2.950, Val loss 4.570\n",
            "Ep 8 (Step 007160): Train loss 3.102, Val loss 4.577\n",
            "Ep 8 (Step 007165): Train loss 3.306, Val loss 4.551\n",
            "Ep 8 (Step 007170): Train loss 2.832, Val loss 4.581\n",
            "Ep 8 (Step 007175): Train loss 3.105, Val loss 4.573\n",
            "Ep 8 (Step 007180): Train loss 3.087, Val loss 4.552\n",
            "Ep 8 (Step 007185): Train loss 3.077, Val loss 4.540\n",
            "Ep 8 (Step 007190): Train loss 3.227, Val loss 4.552\n",
            "Ep 8 (Step 007195): Train loss 2.801, Val loss 4.562\n",
            "Ep 8 (Step 007200): Train loss 3.263, Val loss 4.564\n",
            "Ep 8 (Step 007205): Train loss 3.288, Val loss 4.558\n",
            "Ep 8 (Step 007210): Train loss 2.931, Val loss 4.546\n",
            "Ep 8 (Step 007215): Train loss 3.085, Val loss 4.541\n",
            "Ep 8 (Step 007220): Train loss 3.426, Val loss 4.567\n",
            "Ep 8 (Step 007225): Train loss 3.378, Val loss 4.556\n",
            "Ep 8 (Step 007230): Train loss 3.010, Val loss 4.526\n",
            "Ep 8 (Step 007235): Train loss 3.340, Val loss 4.529\n",
            "Ep 8 (Step 007240): Train loss 3.204, Val loss 4.538\n",
            "Ep 8 (Step 007245): Train loss 2.908, Val loss 4.528\n",
            "Ep 8 (Step 007250): Train loss 3.437, Val loss 4.537\n",
            "Ep 8 (Step 007255): Train loss 2.764, Val loss 4.530\n",
            "Ep 8 (Step 007260): Train loss 3.098, Val loss 4.529\n",
            "He said we came here. We all had done, and we shall not know that we shall have been where we were to-night.  You are to-morrow, said he, that you are not have\n",
            "Ep 9 (Step 007265): Train loss 3.383, Val loss 4.510\n",
            "Ep 9 (Step 007270): Train loss 3.351, Val loss 4.515\n",
            "Ep 9 (Step 007275): Train loss 3.058, Val loss 4.532\n",
            "Ep 9 (Step 007280): Train loss 3.244, Val loss 4.523\n",
            "Ep 9 (Step 007285): Train loss 2.837, Val loss 4.526\n",
            "Ep 9 (Step 007290): Train loss 3.253, Val loss 4.512\n",
            "Ep 9 (Step 007295): Train loss 3.029, Val loss 4.492\n",
            "Ep 9 (Step 007300): Train loss 2.938, Val loss 4.538\n",
            "Ep 9 (Step 007305): Train loss 3.429, Val loss 4.567\n",
            "Ep 9 (Step 007310): Train loss 3.252, Val loss 4.547\n",
            "Ep 9 (Step 007315): Train loss 2.820, Val loss 4.550\n",
            "Ep 9 (Step 007320): Train loss 3.094, Val loss 4.545\n",
            "Ep 9 (Step 007325): Train loss 3.052, Val loss 4.525\n",
            "Ep 9 (Step 007330): Train loss 2.785, Val loss 4.552\n",
            "Ep 9 (Step 007335): Train loss 3.021, Val loss 4.525\n",
            "Ep 9 (Step 007340): Train loss 3.162, Val loss 4.534\n",
            "Ep 9 (Step 007345): Train loss 2.837, Val loss 4.556\n",
            "Ep 9 (Step 007350): Train loss 3.256, Val loss 4.525\n",
            "Ep 9 (Step 007355): Train loss 3.002, Val loss 4.547\n",
            "Ep 9 (Step 007360): Train loss 3.309, Val loss 4.536\n",
            "Ep 9 (Step 007365): Train loss 2.805, Val loss 4.538\n",
            "Ep 9 (Step 007370): Train loss 2.798, Val loss 4.526\n",
            "Ep 9 (Step 007375): Train loss 3.211, Val loss 4.533\n",
            "Ep 9 (Step 007380): Train loss 2.701, Val loss 4.520\n",
            "Ep 9 (Step 007385): Train loss 3.285, Val loss 4.541\n",
            "Ep 9 (Step 007390): Train loss 3.421, Val loss 4.554\n",
            "Ep 9 (Step 007395): Train loss 3.070, Val loss 4.564\n",
            "Ep 9 (Step 007400): Train loss 2.777, Val loss 4.571\n",
            "Ep 9 (Step 007405): Train loss 2.909, Val loss 4.576\n",
            "Ep 9 (Step 007410): Train loss 3.326, Val loss 4.517\n",
            "Ep 9 (Step 007415): Train loss 3.188, Val loss 4.520\n",
            "Ep 9 (Step 007420): Train loss 2.839, Val loss 4.549\n",
            "Ep 9 (Step 007425): Train loss 3.060, Val loss 4.564\n",
            "Ep 9 (Step 007430): Train loss 3.279, Val loss 4.551\n",
            "Ep 9 (Step 007435): Train loss 2.285, Val loss 4.556\n",
            "Ep 9 (Step 007440): Train loss 3.523, Val loss 4.575\n",
            "Ep 9 (Step 007445): Train loss 3.069, Val loss 4.590\n",
            "Ep 9 (Step 007450): Train loss 3.028, Val loss 4.574\n",
            "Ep 9 (Step 007455): Train loss 2.557, Val loss 4.542\n",
            "Ep 9 (Step 007460): Train loss 3.612, Val loss 4.555\n",
            "Ep 9 (Step 007465): Train loss 2.684, Val loss 4.553\n",
            "Ep 9 (Step 007470): Train loss 3.047, Val loss 4.539\n",
            "Ep 9 (Step 007475): Train loss 2.635, Val loss 4.514\n",
            "Ep 9 (Step 007480): Train loss 2.986, Val loss 4.513\n",
            "Ep 9 (Step 007485): Train loss 3.115, Val loss 4.512\n",
            "Ep 9 (Step 007490): Train loss 3.134, Val loss 4.613\n",
            "Ep 9 (Step 007495): Train loss 3.437, Val loss 4.579\n",
            "Ep 9 (Step 007500): Train loss 3.362, Val loss 4.553\n",
            "Ep 9 (Step 007505): Train loss 3.055, Val loss 4.582\n",
            "Ep 9 (Step 007510): Train loss 3.185, Val loss 4.576\n",
            "Ep 9 (Step 007515): Train loss 3.292, Val loss 4.534\n",
            "Ep 9 (Step 007520): Train loss 3.128, Val loss 4.550\n",
            "Ep 9 (Step 007525): Train loss 3.221, Val loss 4.546\n",
            "Ep 9 (Step 007530): Train loss 3.179, Val loss 4.548\n",
            "Ep 9 (Step 007535): Train loss 3.185, Val loss 4.585\n",
            "Ep 9 (Step 007540): Train loss 2.912, Val loss 4.563\n",
            "Ep 9 (Step 007545): Train loss 2.893, Val loss 4.619\n",
            "Ep 9 (Step 007550): Train loss 3.120, Val loss 4.654\n",
            "Ep 9 (Step 007555): Train loss 3.303, Val loss 4.611\n",
            "Ep 9 (Step 007560): Train loss 2.844, Val loss 4.563\n",
            "Ep 9 (Step 007565): Train loss 2.851, Val loss 4.579\n",
            "Ep 9 (Step 007570): Train loss 3.220, Val loss 4.596\n",
            "Ep 9 (Step 007575): Train loss 3.272, Val loss 4.577\n",
            "Ep 9 (Step 007580): Train loss 2.680, Val loss 4.589\n",
            "Ep 9 (Step 007585): Train loss 3.043, Val loss 4.607\n",
            "Ep 9 (Step 007590): Train loss 3.242, Val loss 4.550\n",
            "Ep 9 (Step 007595): Train loss 2.958, Val loss 4.556\n",
            "Ep 9 (Step 007600): Train loss 2.686, Val loss 4.571\n",
            "Ep 9 (Step 007605): Train loss 3.270, Val loss 4.590\n",
            "Ep 9 (Step 007610): Train loss 3.071, Val loss 4.559\n",
            "Ep 9 (Step 007615): Train loss 3.266, Val loss 4.567\n",
            "Ep 9 (Step 007620): Train loss 3.096, Val loss 4.553\n",
            "Ep 9 (Step 007625): Train loss 3.554, Val loss 4.536\n",
            "Ep 9 (Step 007630): Train loss 2.956, Val loss 4.577\n",
            "Ep 9 (Step 007635): Train loss 3.129, Val loss 4.551\n",
            "Ep 9 (Step 007640): Train loss 3.501, Val loss 4.557\n",
            "Ep 9 (Step 007645): Train loss 2.946, Val loss 4.602\n",
            "Ep 9 (Step 007650): Train loss 2.428, Val loss 4.568\n",
            "Ep 9 (Step 007655): Train loss 3.240, Val loss 4.529\n",
            "Ep 9 (Step 007660): Train loss 3.075, Val loss 4.541\n",
            "Ep 9 (Step 007665): Train loss 3.613, Val loss 4.546\n",
            "Ep 9 (Step 007670): Train loss 3.361, Val loss 4.536\n",
            "Ep 9 (Step 007675): Train loss 2.842, Val loss 4.540\n",
            "Ep 9 (Step 007680): Train loss 2.697, Val loss 4.541\n",
            "Ep 9 (Step 007685): Train loss 3.358, Val loss 4.562\n",
            "Ep 9 (Step 007690): Train loss 3.450, Val loss 4.544\n",
            "Ep 9 (Step 007695): Train loss 2.987, Val loss 4.555\n",
            "Ep 9 (Step 007700): Train loss 2.919, Val loss 4.539\n",
            "Ep 9 (Step 007705): Train loss 3.080, Val loss 4.539\n",
            "Ep 9 (Step 007710): Train loss 3.217, Val loss 4.537\n",
            "Ep 9 (Step 007715): Train loss 2.910, Val loss 4.574\n",
            "Ep 9 (Step 007720): Train loss 3.204, Val loss 4.622\n",
            "Ep 9 (Step 007725): Train loss 3.194, Val loss 4.612\n",
            "Ep 9 (Step 007730): Train loss 2.937, Val loss 4.622\n",
            "Ep 9 (Step 007735): Train loss 3.238, Val loss 4.640\n",
            "Ep 9 (Step 007740): Train loss 3.255, Val loss 4.612\n",
            "Ep 9 (Step 007745): Train loss 3.288, Val loss 4.591\n",
            "Ep 9 (Step 007750): Train loss 2.693, Val loss 4.548\n",
            "Ep 9 (Step 007755): Train loss 2.725, Val loss 4.576\n",
            "Ep 9 (Step 007760): Train loss 2.717, Val loss 4.609\n",
            "Ep 9 (Step 007765): Train loss 3.520, Val loss 4.559\n",
            "Ep 9 (Step 007770): Train loss 3.101, Val loss 4.532\n",
            "Ep 9 (Step 007775): Train loss 3.762, Val loss 4.552\n",
            "Ep 9 (Step 007780): Train loss 3.047, Val loss 4.533\n",
            "Ep 9 (Step 007785): Train loss 3.139, Val loss 4.533\n",
            "Ep 9 (Step 007790): Train loss 3.177, Val loss 4.550\n",
            "Ep 9 (Step 007795): Train loss 3.104, Val loss 4.516\n",
            "Ep 9 (Step 007800): Train loss 3.370, Val loss 4.525\n",
            "Ep 9 (Step 007805): Train loss 3.095, Val loss 4.536\n",
            "Ep 9 (Step 007810): Train loss 2.917, Val loss 4.552\n",
            "Ep 9 (Step 007815): Train loss 2.674, Val loss 4.546\n",
            "Ep 9 (Step 007820): Train loss 2.889, Val loss 4.536\n",
            "Ep 9 (Step 007825): Train loss 3.413, Val loss 4.575\n",
            "Ep 9 (Step 007830): Train loss 2.920, Val loss 4.573\n",
            "Ep 9 (Step 007835): Train loss 3.217, Val loss 4.574\n",
            "Ep 9 (Step 007840): Train loss 3.096, Val loss 4.571\n",
            "Ep 9 (Step 007845): Train loss 2.925, Val loss 4.547\n",
            "Ep 9 (Step 007850): Train loss 3.178, Val loss 4.561\n",
            "Ep 9 (Step 007855): Train loss 2.620, Val loss 4.551\n",
            "Ep 9 (Step 007860): Train loss 3.049, Val loss 4.531\n",
            "Ep 9 (Step 007865): Train loss 2.906, Val loss 4.532\n",
            "Ep 9 (Step 007870): Train loss 3.145, Val loss 4.551\n",
            "Ep 9 (Step 007875): Train loss 3.164, Val loss 4.572\n",
            "Ep 9 (Step 007880): Train loss 3.377, Val loss 4.559\n",
            "Ep 9 (Step 007885): Train loss 2.574, Val loss 4.579\n",
            "Ep 9 (Step 007890): Train loss 2.853, Val loss 4.561\n",
            "Ep 9 (Step 007895): Train loss 3.089, Val loss 4.536\n",
            "Ep 9 (Step 007900): Train loss 3.053, Val loss 4.558\n",
            "Ep 9 (Step 007905): Train loss 2.944, Val loss 4.584\n",
            "Ep 9 (Step 007910): Train loss 2.518, Val loss 4.601\n",
            "Ep 9 (Step 007915): Train loss 3.136, Val loss 4.607\n",
            "Ep 9 (Step 007920): Train loss 2.506, Val loss 4.599\n",
            "Ep 9 (Step 007925): Train loss 2.865, Val loss 4.574\n",
            "Ep 9 (Step 007930): Train loss 3.103, Val loss 4.605\n",
            "Ep 9 (Step 007935): Train loss 2.644, Val loss 4.583\n",
            "Ep 9 (Step 007940): Train loss 3.152, Val loss 4.553\n",
            "Ep 9 (Step 007945): Train loss 3.246, Val loss 4.589\n",
            "Ep 9 (Step 007950): Train loss 3.344, Val loss 4.599\n",
            "Ep 9 (Step 007955): Train loss 2.855, Val loss 4.619\n",
            "Ep 9 (Step 007960): Train loss 3.477, Val loss 4.625\n",
            "Ep 9 (Step 007965): Train loss 2.977, Val loss 4.637\n",
            "Ep 9 (Step 007970): Train loss 3.465, Val loss 4.609\n",
            "Ep 9 (Step 007975): Train loss 2.981, Val loss 4.634\n",
            "Ep 9 (Step 007980): Train loss 3.044, Val loss 4.602\n",
            "Ep 9 (Step 007985): Train loss 2.897, Val loss 4.603\n",
            "Ep 9 (Step 007990): Train loss 2.744, Val loss 4.595\n",
            "Ep 9 (Step 007995): Train loss 2.684, Val loss 4.584\n",
            "Ep 9 (Step 008000): Train loss 2.822, Val loss 4.574\n",
            "Ep 9 (Step 008005): Train loss 3.017, Val loss 4.597\n",
            "Ep 9 (Step 008010): Train loss 2.873, Val loss 4.604\n",
            "Ep 9 (Step 008015): Train loss 2.620, Val loss 4.637\n",
            "Ep 9 (Step 008020): Train loss 2.511, Val loss 4.587\n",
            "Ep 9 (Step 008025): Train loss 3.031, Val loss 4.557\n",
            "Ep 9 (Step 008030): Train loss 2.713, Val loss 4.608\n",
            "Ep 9 (Step 008035): Train loss 3.016, Val loss 4.621\n",
            "Ep 9 (Step 008040): Train loss 2.742, Val loss 4.595\n",
            "Ep 9 (Step 008045): Train loss 2.887, Val loss 4.613\n",
            "Ep 9 (Step 008050): Train loss 2.965, Val loss 4.607\n",
            "Ep 9 (Step 008055): Train loss 3.333, Val loss 4.590\n",
            "Ep 9 (Step 008060): Train loss 2.709, Val loss 4.605\n",
            "Ep 9 (Step 008065): Train loss 2.949, Val loss 4.598\n",
            "Ep 9 (Step 008070): Train loss 3.043, Val loss 4.585\n",
            "Ep 9 (Step 008075): Train loss 2.968, Val loss 4.601\n",
            "Ep 9 (Step 008080): Train loss 3.281, Val loss 4.602\n",
            "Ep 9 (Step 008085): Train loss 2.855, Val loss 4.585\n",
            "Ep 9 (Step 008090): Train loss 3.058, Val loss 4.560\n",
            "Ep 9 (Step 008095): Train loss 2.928, Val loss 4.557\n",
            "Ep 9 (Step 008100): Train loss 2.635, Val loss 4.600\n",
            "Ep 9 (Step 008105): Train loss 3.340, Val loss 4.640\n",
            "Ep 9 (Step 008110): Train loss 3.090, Val loss 4.648\n",
            "Ep 9 (Step 008115): Train loss 3.153, Val loss 4.654\n",
            "Ep 9 (Step 008120): Train loss 3.281, Val loss 4.621\n",
            "Ep 9 (Step 008125): Train loss 2.761, Val loss 4.625\n",
            "Ep 9 (Step 008130): Train loss 3.063, Val loss 4.624\n",
            "Ep 9 (Step 008135): Train loss 2.871, Val loss 4.623\n",
            "Ep 9 (Step 008140): Train loss 2.802, Val loss 4.640\n",
            "Ep 9 (Step 008145): Train loss 2.609, Val loss 4.621\n",
            "Ep 9 (Step 008150): Train loss 2.668, Val loss 4.611\n",
            "Ep 9 (Step 008155): Train loss 3.008, Val loss 4.606\n",
            "Ep 9 (Step 008160): Train loss 3.140, Val loss 4.584\n",
            "Ep 9 (Step 008165): Train loss 3.033, Val loss 4.557\n",
            "Ep 9 (Step 008170): Train loss 2.928, Val loss 4.585\n",
            "He said we came here. We have a word, and saw us that we were all night. We have no one of the Professors have been in the way, and we have had all the house. I had not to see the time of\n",
            "Ep 10 (Step 008175): Train loss 2.974, Val loss 4.679\n",
            "Ep 10 (Step 008180): Train loss 2.408, Val loss 4.661\n",
            "Ep 10 (Step 008185): Train loss 2.827, Val loss 4.579\n",
            "Ep 10 (Step 008190): Train loss 2.654, Val loss 4.587\n",
            "Ep 10 (Step 008195): Train loss 2.924, Val loss 4.624\n",
            "Ep 10 (Step 008200): Train loss 3.128, Val loss 4.615\n",
            "Ep 10 (Step 008205): Train loss 3.049, Val loss 4.605\n",
            "Ep 10 (Step 008210): Train loss 3.010, Val loss 4.580\n",
            "Ep 10 (Step 008215): Train loss 2.944, Val loss 4.568\n",
            "Ep 10 (Step 008220): Train loss 2.798, Val loss 4.623\n",
            "Ep 10 (Step 008225): Train loss 2.917, Val loss 4.660\n",
            "Ep 10 (Step 008230): Train loss 2.565, Val loss 4.635\n",
            "Ep 10 (Step 008235): Train loss 2.813, Val loss 4.654\n",
            "Ep 10 (Step 008240): Train loss 2.970, Val loss 4.634\n",
            "Ep 10 (Step 008245): Train loss 2.751, Val loss 4.645\n",
            "Ep 10 (Step 008250): Train loss 2.917, Val loss 4.636\n",
            "Ep 10 (Step 008255): Train loss 2.643, Val loss 4.609\n",
            "Ep 10 (Step 008260): Train loss 3.046, Val loss 4.635\n",
            "Ep 10 (Step 008265): Train loss 2.931, Val loss 4.616\n",
            "Ep 10 (Step 008270): Train loss 2.923, Val loss 4.592\n",
            "Ep 10 (Step 008275): Train loss 2.781, Val loss 4.580\n",
            "Ep 10 (Step 008280): Train loss 2.895, Val loss 4.580\n",
            "Ep 10 (Step 008285): Train loss 2.517, Val loss 4.589\n",
            "Ep 10 (Step 008290): Train loss 2.937, Val loss 4.596\n",
            "Ep 10 (Step 008295): Train loss 2.717, Val loss 4.625\n",
            "Ep 10 (Step 008300): Train loss 3.110, Val loss 4.589\n",
            "Ep 10 (Step 008305): Train loss 2.838, Val loss 4.592\n",
            "Ep 10 (Step 008310): Train loss 2.360, Val loss 4.605\n",
            "Ep 10 (Step 008315): Train loss 2.841, Val loss 4.578\n",
            "Ep 10 (Step 008320): Train loss 2.978, Val loss 4.612\n",
            "Ep 10 (Step 008325): Train loss 3.169, Val loss 4.610\n",
            "Ep 10 (Step 008330): Train loss 2.893, Val loss 4.601\n",
            "Ep 10 (Step 008335): Train loss 2.766, Val loss 4.609\n",
            "Ep 10 (Step 008340): Train loss 2.406, Val loss 4.597\n",
            "Ep 10 (Step 008345): Train loss 2.932, Val loss 4.625\n",
            "Ep 10 (Step 008350): Train loss 2.865, Val loss 4.638\n",
            "Ep 10 (Step 008355): Train loss 2.678, Val loss 4.619\n",
            "Ep 10 (Step 008360): Train loss 2.423, Val loss 4.605\n",
            "Ep 10 (Step 008365): Train loss 2.544, Val loss 4.587\n",
            "Ep 10 (Step 008370): Train loss 3.054, Val loss 4.608\n",
            "Ep 10 (Step 008375): Train loss 2.909, Val loss 4.650\n",
            "Ep 10 (Step 008380): Train loss 2.460, Val loss 4.587\n",
            "Ep 10 (Step 008385): Train loss 2.901, Val loss 4.581\n",
            "Ep 10 (Step 008390): Train loss 2.723, Val loss 4.654\n",
            "Ep 10 (Step 008395): Train loss 2.597, Val loss 4.579\n",
            "Ep 10 (Step 008400): Train loss 3.103, Val loss 4.595\n",
            "Ep 10 (Step 008405): Train loss 2.839, Val loss 4.646\n",
            "Ep 10 (Step 008410): Train loss 2.654, Val loss 4.631\n",
            "Ep 10 (Step 008415): Train loss 2.993, Val loss 4.609\n",
            "Ep 10 (Step 008420): Train loss 2.717, Val loss 4.594\n",
            "Ep 10 (Step 008425): Train loss 3.035, Val loss 4.593\n",
            "Ep 10 (Step 008430): Train loss 2.477, Val loss 4.592\n",
            "Ep 10 (Step 008435): Train loss 2.803, Val loss 4.593\n",
            "Ep 10 (Step 008440): Train loss 2.611, Val loss 4.632\n",
            "Ep 10 (Step 008445): Train loss 2.907, Val loss 4.567\n",
            "Ep 10 (Step 008450): Train loss 2.795, Val loss 4.576\n",
            "Ep 10 (Step 008455): Train loss 2.764, Val loss 4.614\n",
            "Ep 10 (Step 008460): Train loss 2.936, Val loss 4.598\n",
            "Ep 10 (Step 008465): Train loss 2.947, Val loss 4.576\n",
            "Ep 10 (Step 008470): Train loss 3.173, Val loss 4.602\n",
            "Ep 10 (Step 008475): Train loss 2.936, Val loss 4.608\n",
            "Ep 10 (Step 008480): Train loss 2.882, Val loss 4.599\n",
            "Ep 10 (Step 008485): Train loss 2.822, Val loss 4.607\n",
            "Ep 10 (Step 008490): Train loss 3.038, Val loss 4.621\n",
            "Ep 10 (Step 008495): Train loss 2.871, Val loss 4.606\n",
            "Ep 10 (Step 008500): Train loss 2.603, Val loss 4.642\n",
            "Ep 10 (Step 008505): Train loss 2.731, Val loss 4.650\n",
            "Ep 10 (Step 008510): Train loss 2.928, Val loss 4.650\n",
            "Ep 10 (Step 008515): Train loss 2.928, Val loss 4.659\n",
            "Ep 10 (Step 008520): Train loss 2.702, Val loss 4.666\n",
            "Ep 10 (Step 008525): Train loss 3.366, Val loss 4.650\n",
            "Ep 10 (Step 008530): Train loss 3.076, Val loss 4.644\n",
            "Ep 10 (Step 008535): Train loss 2.471, Val loss 4.622\n",
            "Ep 10 (Step 008540): Train loss 3.070, Val loss 4.630\n",
            "Ep 10 (Step 008545): Train loss 2.780, Val loss 4.607\n",
            "Ep 10 (Step 008550): Train loss 2.725, Val loss 4.610\n",
            "Ep 10 (Step 008555): Train loss 2.948, Val loss 4.610\n",
            "Ep 10 (Step 008560): Train loss 2.948, Val loss 4.611\n",
            "Ep 10 (Step 008565): Train loss 2.713, Val loss 4.642\n",
            "Ep 10 (Step 008570): Train loss 2.995, Val loss 4.631\n",
            "Ep 10 (Step 008575): Train loss 2.885, Val loss 4.592\n",
            "Ep 10 (Step 008580): Train loss 2.566, Val loss 4.636\n",
            "Ep 10 (Step 008585): Train loss 2.749, Val loss 4.618\n",
            "Ep 10 (Step 008590): Train loss 2.817, Val loss 4.583\n",
            "Ep 10 (Step 008595): Train loss 3.197, Val loss 4.578\n",
            "Ep 10 (Step 008600): Train loss 2.559, Val loss 4.577\n",
            "Ep 10 (Step 008605): Train loss 2.763, Val loss 4.611\n",
            "Ep 10 (Step 008610): Train loss 3.099, Val loss 4.639\n",
            "Ep 10 (Step 008615): Train loss 2.850, Val loss 4.624\n",
            "Ep 10 (Step 008620): Train loss 2.646, Val loss 4.599\n",
            "Ep 10 (Step 008625): Train loss 2.638, Val loss 4.616\n",
            "Ep 10 (Step 008630): Train loss 2.969, Val loss 4.636\n",
            "Ep 10 (Step 008635): Train loss 2.749, Val loss 4.585\n",
            "Ep 10 (Step 008640): Train loss 2.652, Val loss 4.565\n",
            "Ep 10 (Step 008645): Train loss 2.996, Val loss 4.589\n",
            "Ep 10 (Step 008650): Train loss 3.000, Val loss 4.645\n",
            "Ep 10 (Step 008655): Train loss 3.097, Val loss 4.613\n",
            "Ep 10 (Step 008660): Train loss 2.714, Val loss 4.591\n",
            "Ep 10 (Step 008665): Train loss 3.128, Val loss 4.597\n",
            "Ep 10 (Step 008670): Train loss 2.740, Val loss 4.605\n",
            "Ep 10 (Step 008675): Train loss 3.000, Val loss 4.627\n",
            "Ep 10 (Step 008680): Train loss 2.952, Val loss 4.603\n",
            "Ep 10 (Step 008685): Train loss 3.102, Val loss 4.605\n",
            "Ep 10 (Step 008690): Train loss 2.954, Val loss 4.583\n",
            "Ep 10 (Step 008695): Train loss 2.728, Val loss 4.569\n",
            "Ep 10 (Step 008700): Train loss 2.561, Val loss 4.606\n",
            "Ep 10 (Step 008705): Train loss 3.107, Val loss 4.647\n",
            "Ep 10 (Step 008710): Train loss 2.667, Val loss 4.655\n",
            "Ep 10 (Step 008715): Train loss 2.437, Val loss 4.659\n",
            "Ep 10 (Step 008720): Train loss 3.075, Val loss 4.653\n",
            "Ep 10 (Step 008725): Train loss 2.576, Val loss 4.682\n",
            "Ep 10 (Step 008730): Train loss 2.510, Val loss 4.655\n",
            "Ep 10 (Step 008735): Train loss 2.752, Val loss 4.662\n",
            "Ep 10 (Step 008740): Train loss 2.981, Val loss 4.651\n",
            "Ep 10 (Step 008745): Train loss 2.950, Val loss 4.655\n",
            "Ep 10 (Step 008750): Train loss 2.753, Val loss 4.648\n",
            "Ep 10 (Step 008755): Train loss 2.587, Val loss 4.640\n",
            "Ep 10 (Step 008760): Train loss 2.478, Val loss 4.625\n",
            "Ep 10 (Step 008765): Train loss 2.516, Val loss 4.595\n",
            "Ep 10 (Step 008770): Train loss 2.472, Val loss 4.593\n",
            "Ep 10 (Step 008775): Train loss 2.587, Val loss 4.614\n",
            "Ep 10 (Step 008780): Train loss 3.055, Val loss 4.644\n",
            "Ep 10 (Step 008785): Train loss 2.634, Val loss 4.638\n",
            "Ep 10 (Step 008790): Train loss 2.753, Val loss 4.655\n",
            "Ep 10 (Step 008795): Train loss 2.829, Val loss 4.658\n",
            "Ep 10 (Step 008800): Train loss 2.760, Val loss 4.664\n",
            "Ep 10 (Step 008805): Train loss 2.470, Val loss 4.633\n",
            "Ep 10 (Step 008810): Train loss 2.691, Val loss 4.597\n",
            "Ep 10 (Step 008815): Train loss 2.934, Val loss 4.643\n",
            "Ep 10 (Step 008820): Train loss 2.715, Val loss 4.731\n",
            "Ep 10 (Step 008825): Train loss 2.952, Val loss 4.690\n",
            "Ep 10 (Step 008830): Train loss 2.636, Val loss 4.658\n",
            "Ep 10 (Step 008835): Train loss 2.635, Val loss 4.715\n",
            "Ep 10 (Step 008840): Train loss 2.641, Val loss 4.689\n",
            "Ep 10 (Step 008845): Train loss 2.905, Val loss 4.644\n",
            "Ep 10 (Step 008850): Train loss 2.627, Val loss 4.637\n",
            "Ep 10 (Step 008855): Train loss 3.117, Val loss 4.625\n",
            "Ep 10 (Step 008860): Train loss 2.653, Val loss 4.656\n",
            "Ep 10 (Step 008865): Train loss 2.638, Val loss 4.678\n",
            "Ep 10 (Step 008870): Train loss 2.837, Val loss 4.617\n",
            "Ep 10 (Step 008875): Train loss 2.909, Val loss 4.610\n",
            "Ep 10 (Step 008880): Train loss 2.306, Val loss 4.645\n",
            "Ep 10 (Step 008885): Train loss 2.298, Val loss 4.694\n",
            "Ep 10 (Step 008890): Train loss 2.846, Val loss 4.678\n",
            "Ep 10 (Step 008895): Train loss 2.883, Val loss 4.620\n",
            "Ep 10 (Step 008900): Train loss 2.568, Val loss 4.615\n",
            "Ep 10 (Step 008905): Train loss 2.717, Val loss 4.610\n",
            "Ep 10 (Step 008910): Train loss 2.348, Val loss 4.614\n",
            "Ep 10 (Step 008915): Train loss 2.459, Val loss 4.611\n",
            "Ep 10 (Step 008920): Train loss 2.598, Val loss 4.614\n",
            "Ep 10 (Step 008925): Train loss 2.560, Val loss 4.628\n",
            "Ep 10 (Step 008930): Train loss 2.887, Val loss 4.630\n",
            "Ep 10 (Step 008935): Train loss 2.523, Val loss 4.629\n",
            "Ep 10 (Step 008940): Train loss 2.207, Val loss 4.655\n",
            "Ep 10 (Step 008945): Train loss 2.687, Val loss 4.641\n",
            "Ep 10 (Step 008950): Train loss 2.479, Val loss 4.658\n",
            "Ep 10 (Step 008955): Train loss 2.559, Val loss 4.674\n",
            "Ep 10 (Step 008960): Train loss 2.590, Val loss 4.661\n",
            "Ep 10 (Step 008965): Train loss 2.517, Val loss 4.628\n",
            "Ep 10 (Step 008970): Train loss 2.695, Val loss 4.692\n",
            "Ep 10 (Step 008975): Train loss 2.786, Val loss 4.691\n",
            "Ep 10 (Step 008980): Train loss 2.884, Val loss 4.683\n",
            "Ep 10 (Step 008985): Train loss 2.762, Val loss 4.667\n",
            "Ep 10 (Step 008990): Train loss 2.784, Val loss 4.653\n",
            "Ep 10 (Step 008995): Train loss 2.873, Val loss 4.626\n",
            "Ep 10 (Step 009000): Train loss 2.686, Val loss 4.657\n",
            "Ep 10 (Step 009005): Train loss 2.129, Val loss 4.700\n",
            "Ep 10 (Step 009010): Train loss 2.919, Val loss 4.665\n",
            "Ep 10 (Step 009015): Train loss 2.040, Val loss 4.669\n",
            "Ep 10 (Step 009020): Train loss 2.541, Val loss 4.684\n",
            "Ep 10 (Step 009025): Train loss 2.814, Val loss 4.657\n",
            "Ep 10 (Step 009030): Train loss 2.786, Val loss 4.694\n",
            "Ep 10 (Step 009035): Train loss 2.465, Val loss 4.654\n",
            "Ep 10 (Step 009040): Train loss 2.756, Val loss 4.631\n",
            "Ep 10 (Step 009045): Train loss 2.789, Val loss 4.633\n",
            "Ep 10 (Step 009050): Train loss 2.364, Val loss 4.626\n",
            "Ep 10 (Step 009055): Train loss 2.446, Val loss 4.644\n",
            "Ep 10 (Step 009060): Train loss 2.618, Val loss 4.684\n",
            "Ep 10 (Step 009065): Train loss 2.666, Val loss 4.696\n",
            "Ep 10 (Step 009070): Train loss 2.857, Val loss 4.685\n",
            "Ep 10 (Step 009075): Train loss 2.423, Val loss 4.652\n",
            "He said we came here. We not to us, for us a few days, and we must be ready to himself. We must have been the box, for we have been able to-night, and we must have been to-morrow. \n",
            "Sample output:\n",
            " The tree was a very kind of\n",
            "dance, and, the nose of the\n",
            "other.\n",
            "\n",
            "Look!\n",
            "\n",
            "All training runs completed in 17.82 minutes.\n"
          ]
        }
      ]
    }
  ]
}